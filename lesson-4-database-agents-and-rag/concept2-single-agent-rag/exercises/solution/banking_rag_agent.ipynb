{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718056e4",
   "metadata": {},
   "source": [
    "# Single Agent RAG: Banking Policy Assistant\n",
    "\n",
    "**Objective**: Build a single-agent RAG system that retrieves banking policy information from a local Chroma vector store and provides accurate answers with citations.\n",
    "\n",
    "**Key Features**:\n",
    "- ðŸ” Parse user queries to extract intent and entities\n",
    "- ðŸ“š Retrieve relevant documents from Chroma vector store\n",
    "- ðŸŽ¯ Generate answers with explicit citations\n",
    "- ðŸ”„ Re-retrieve if initial evidence is insufficient\n",
    "- ðŸ›¡ï¸ Handle insufficient evidence gracefully\n",
    "\n",
    "**Time**: ~15-20 minutes\n",
    "\n",
    "**Scenario**: Support a bank's customer service team with instant access to policy information for fees, limits, and procedures.\n",
    "\n",
    "## ðŸ“Š Banking Knowledge Base\n",
    "\n",
    "Our vector store contains comprehensive banking policies:\n",
    "- Personal Checking Fee Schedule (2025)\n",
    "- Domestic & International Wire Transfer Policy (2025)  \n",
    "- ATM Network & Reimbursement Rules (2025)\n",
    "- Overdraft & NSF Policy (Rev. Apr 2025)\n",
    "- Mobile Deposit Limits & Hold Periods (Consumer)\n",
    "- Interest Tiers & APY Disclosures (Savings/MMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3d9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Environment Setup:\n",
      "   âœ… OpenAI API Key: âœ“ Configured\n",
      "   ðŸ“š Knowledge Base: Banking policy documents\n",
      "   ðŸ¤– Simple RAG with direct OpenAI + Chroma integration\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI for LLM and embeddings\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Chroma for vector storage\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"ðŸ”§ Environment Setup:\")\n",
    "print(f\"   âœ… OpenAI API Key: {'âœ“ Configured' if os.getenv('OPENAI_API_KEY') else 'âŒ Missing'}\")\n",
    "print(\"   ðŸ“š Knowledge Base: Banking policy documents\") \n",
    "print(\"   ðŸ¤– Simple RAG with direct OpenAI + Chroma integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cc280",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Data Models for RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c6cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Simple data model defined for minimal RAG\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SimpleRAGResponse:\n",
    "    \"\"\"Simple response from our minimal RAG agent\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "    retrieved_chunks: int\n",
    "    needed_retry: bool\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "\n",
    "print(\"ðŸ“‹ Simple data model defined for minimal RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc42d4",
   "metadata": {},
   "source": [
    "## ðŸ“š Knowledge Base Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e846fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Setting up simple RAG system...\n",
      "ðŸ“š Using existing collection: banking_policies_simple\n",
      "   ðŸ“Š Documents in collection: 7\n",
      "âœ… Simple RAG system ready!\n",
      "   ðŸ“Š Collection: 7 document chunks\n"
     ]
    }
   ],
   "source": [
    "def load_banking_documents_to_chroma():\n",
    "    \"\"\"Load banking documents directly into Chroma with embeddings\"\"\"\n",
    "    \n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    collection_name = \"banking_policies_simple\"\n",
    "    \n",
    "    # Check if collection exists\n",
    "    try:\n",
    "        existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"ðŸ“š Using existing collection: {collection_name}\")\n",
    "            collection = chroma_client.get_collection(collection_name)\n",
    "            print(f\"   ðŸ“Š Documents in collection: {collection.count()}\")\n",
    "            return collection\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    print(f\"ðŸ“ Creating new collection: {collection_name}\")\n",
    "    \n",
    "    # Create new collection\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"description\": \"Simple banking policy documents\"}\n",
    "    )\n",
    "    \n",
    "    # Load documents from data directory\n",
    "    data_dir = \"./data\"\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"âŒ Data directory '{data_dir}' not found!\")\n",
    "        return collection\n",
    "    \n",
    "    doc_id = 0\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Split content into chunks\n",
    "                chunks = split_text_into_chunks(content, chunk_size=500, overlap=50)\n",
    "                \n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    documents.append(chunk)\n",
    "                    metadatas.append({\n",
    "                        'filename': filename,\n",
    "                        'doc_title': filename.replace('_', ' ').replace('.txt', '').title(),\n",
    "                        'chunk_id': f\"{filename}_{i}\"\n",
    "                    })\n",
    "                    ids.append(f\"doc_{doc_id}\")\n",
    "                    doc_id += 1\n",
    "                \n",
    "                print(f\"   ðŸ“„ Loaded: {filename} ({len(chunks)} chunks)\")\n",
    "    \n",
    "    if documents:\n",
    "        # Add documents to collection (Chroma will automatically generate embeddings)\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"ðŸŽ‰ Added {len(documents)} document chunks to Chroma\")\n",
    "    else:\n",
    "        print(\"âŒ No documents found to add\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Simple text chunking function\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get OpenAI embedding for text\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Setup the knowledge base\n",
    "print(\"ðŸš€ Setting up simple RAG system...\")\n",
    "chroma_collection = load_banking_documents_to_chroma()\n",
    "\n",
    "if chroma_collection.count() > 0:\n",
    "    print(\"âœ… Simple RAG system ready!\")\n",
    "    print(f\"   ðŸ“Š Collection: {chroma_collection.count()} document chunks\")\n",
    "else:\n",
    "    print(\"âŒ Failed to setup RAG system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837be16",
   "metadata": {},
   "source": [
    "## ðŸ¤– Single Agent RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d05afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Simple autonomous RAG agent ready!\n",
      "   ðŸ§  LLM decides when to re-search\n",
      "   ðŸ”„ Self-improving search queries\n",
      "   ðŸ“Š No complex intent parsing needed\n"
     ]
    }
   ],
   "source": [
    "class SimpleRAGAgent:\n",
    "    \"\"\"Minimal autonomous RAG agent that thinks for itself\"\"\"\n",
    "    \n",
    "    def __init__(self, chroma_collection):\n",
    "        self.collection = chroma_collection\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve_documents(self, query: str, n_results: int = 4) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant documents from Chroma\"\"\"\n",
    "        \n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'documents': results['documents'][0] if results['documents'] else [],\n",
    "            'metadatas': results['metadatas'][0] if results['metadatas'] else [],\n",
    "            'distances': results['distances'][0] if results['distances'] else [],\n",
    "            'count': len(results['documents'][0]) if results['documents'] else 0\n",
    "        }\n",
    "    \n",
    "    def ask_llm_for_answer(self, query: str, context_docs: List[str]) -> str:\n",
    "        \"\"\"Ask the LLM to answer based on retrieved documents\"\"\"\n",
    "        \n",
    "        # Build context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful banking customer service assistant. Answer the customer's question based ONLY on the provided documents.\n",
    "\n",
    "RETRIEVED DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "CUSTOMER QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer based ONLY on the information in the documents above\n",
    "- If the documents contain the answer, provide a clear, helpful response\n",
    "- If the documents don't contain enough information, say \"I don't have enough information in my knowledge base to answer that question\"\n",
    "- Include specific details like fees, limits, times, etc. when available\n",
    "- Be conversational but professional\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def should_retry_retrieval(self, query: str, answer: str, retrieved_docs: List[str]) -> bool:\n",
    "        \"\"\"Let the LLM decide if we need to search again\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating whether a RAG system needs to search again for better information.\n",
    "\n",
    "ORIGINAL QUESTION: {query}\n",
    "\n",
    "CURRENT ANSWER: {answer}\n",
    "\n",
    "RETRIEVED DOCUMENTS: {len(retrieved_docs)} documents found\n",
    "\n",
    "EVALUATION TASK:\n",
    "Respond with ONLY \"YES\" or \"NO\" to indicate if the system should search again.\n",
    "\n",
    "Say \"YES\" if:\n",
    "- The answer says there's not enough information\n",
    "- The answer is vague or incomplete\n",
    "- The question asks for specific details (fees, limits, times) but the answer doesn't provide them\n",
    "\n",
    "Say \"NO\" if:\n",
    "- The answer provides specific, helpful information\n",
    "- The answer appropriately explains what's not available\n",
    "\n",
    "DECISION:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        decision = response.choices[0].message.content.strip().upper()\n",
    "        return decision == \"YES\"\n",
    "    \n",
    "    def generate_better_search_query(self, original_query: str, previous_answer: str) -> str:\n",
    "        \"\"\"Let the LLM generate a better search query for retry\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You need to generate a better search query for a banking knowledge base.\n",
    "\n",
    "ORIGINAL QUERY: {original_query}\n",
    "PREVIOUS ANSWER: {previous_answer}\n",
    "\n",
    "The previous search didn't find good information. Generate a NEW search query that might find better results.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use different keywords or synonyms\n",
    "- Try broader or more specific terms\n",
    "- Focus on key banking concepts\n",
    "- Keep it under 10 words\n",
    "\n",
    "NEW SEARCH QUERY:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def assess_confidence(self, answer: str, retrieved_count: int, best_distance: float) -> str:\n",
    "        \"\"\"Simple confidence assessment\"\"\"\n",
    "        if \"don't have enough information\" in answer.lower():\n",
    "            return \"low\"\n",
    "        elif retrieved_count >= 3 and best_distance < 0.8:\n",
    "            return \"high\"\n",
    "        elif retrieved_count >= 2:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def process_query(self, query: str, show_thinking: bool = False) -> SimpleRAGResponse:\n",
    "        \"\"\"Main method - autonomous RAG processing\"\"\"\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"ðŸ¤” Thinking about: {query}\")\n",
    "        \n",
    "        # Step 1: Initial retrieval\n",
    "        results = self.retrieve_documents(query, n_results=4)\n",
    "        docs = results['documents']\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"ðŸ” Found {results['count']} documents\")\n",
    "        \n",
    "        # Step 2: Generate initial answer\n",
    "        answer = self.ask_llm_for_answer(query, docs)\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"ðŸ’­ Initial answer: {answer[:100]}...\")\n",
    "        \n",
    "        # Step 3: Let LLM decide if we need to retry\n",
    "        needs_retry = False\n",
    "        total_chunks = results['count']\n",
    "        \n",
    "        if results['count'] > 0:\n",
    "            needs_retry = self.should_retry_retrieval(query, answer, docs)\n",
    "        \n",
    "        # Step 4: Retry if needed\n",
    "        if needs_retry:\n",
    "            if show_thinking:\n",
    "                print(f\"ðŸ”„ LLM thinks we should search again...\")\n",
    "            \n",
    "            # Generate better search query\n",
    "            better_query = self.generate_better_search_query(query, answer)\n",
    "            if show_thinking:\n",
    "                print(f\"ðŸŽ¯ Trying: {better_query}\")\n",
    "            \n",
    "            # Search again with more results\n",
    "            retry_results = self.retrieve_documents(better_query, n_results=6)\n",
    "            \n",
    "            if retry_results['count'] > results['count']:\n",
    "                # Use better results\n",
    "                results = retry_results\n",
    "                docs = results['documents']\n",
    "                answer = self.ask_llm_for_answer(query, docs)\n",
    "                total_chunks = retry_results['count']\n",
    "                \n",
    "                if show_thinking:\n",
    "                    print(f\"âœ… Found {retry_results['count']} documents with better query\")\n",
    "        \n",
    "        # Step 5: Extract sources\n",
    "        sources = []\n",
    "        if results['metadatas']:\n",
    "            sources = [meta.get('doc_title', 'Unknown') for meta in results['metadatas']]\n",
    "        \n",
    "        # Step 6: Assess confidence\n",
    "        best_distance = min(results['distances']) if results['distances'] else 1.0\n",
    "        confidence = self.assess_confidence(answer, results['count'], best_distance)\n",
    "        \n",
    "        # Create response\n",
    "        response = SimpleRAGResponse(\n",
    "            query=query,\n",
    "            answer=answer,\n",
    "            sources=list(set(sources)),  # Remove duplicates\n",
    "            retrieved_chunks=total_chunks,\n",
    "            needed_retry=needs_retry,\n",
    "            confidence=confidence\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        self.query_history.append(response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize the simple RAG agent\n",
    "if chroma_collection.count() > 0:\n",
    "    rag_agent = SimpleRAGAgent(chroma_collection)\n",
    "    print(\"ðŸ¤– Simple autonomous RAG agent ready!\")\n",
    "    print(\"   ðŸ§  LLM decides when to re-search\")\n",
    "    print(\"   ðŸ”„ Self-improving search queries\") \n",
    "    print(\"   ðŸ“Š No complex intent parsing needed\")\n",
    "else:\n",
    "    print(\"âŒ Cannot initialize agent - no documents in collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf4996",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test the RAG System\n",
    "\n",
    "Let's test our banking RAG agent with realistic customer service questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b430a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Simple Autonomous RAG Agent\n",
      "\n",
      "============================================================\n",
      "Test 1: What's the same-day domestic wire cut-off time?\n",
      "============================================================\n",
      "ðŸ¤” Thinking about: What's the same-day domestic wire cut-off time?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: The same-day processing cut-off times for domestic wire transfers are as follows: On standard busine...\n",
      "\n",
      "ðŸ“ Answer:\n",
      "The same-day processing cut-off times for domestic wire transfers are as follows: On standard business days, the cut-off time is 3:00 PM ET. On Fridays, the cut-off time is slightly earlier at 2:30 PM ET. Please note that there is no same-day processing on Federal Holidays.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "   1. Personal Checking Fees 2025\n",
      "   2. Overdraft Nsf Policy 2025\n",
      "   3. Wire Transfer Policy 2025\n",
      "   4. Atm Network Reimbursement 2025\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Chunks retrieved: 4\n",
      "   Confidence: medium\n",
      "   Needed retry: No\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 2: How many ATM fee reimbursements per month on Premier Checking?\n",
      "============================================================\n",
      "ðŸ¤” Thinking about: How many ATM fee reimbursements per month on Premier Checking?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: The Premier Checking Account offers up to 6 out-of-network ATM fee reimbursements per month....\n",
      "\n",
      "ðŸ“ Answer:\n",
      "The Premier Checking Account offers up to 6 out-of-network ATM fee reimbursements per month.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "   1. Mobile Deposit Limits 2025\n",
      "   2. Overdraft Nsf Policy 2025\n",
      "   3. Personal Checking Fees 2025\n",
      "   4. Atm Network Reimbursement 2025\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Chunks retrieved: 4\n",
      "   Confidence: high\n",
      "   Needed retry: No\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 3: What's the mobile check deposit limit for new accounts?\n",
      "============================================================\n",
      "ðŸ¤” Thinking about: What's the mobile check deposit limit for new accounts?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: For new customers with accounts that have been open for 0-90 days, the mobile check deposit limits a...\n",
      "\n",
      "ðŸ“ Answer:\n",
      "For new customers with accounts that have been open for 0-90 days, the mobile check deposit limits are as follows: a daily limit of $500, a monthly limit of $2,500, and a single check limit of $500.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "   1. Mobile Deposit Limits 2025\n",
      "   2. Overdraft Nsf Policy 2025\n",
      "   3. Personal Checking Fees 2025\n",
      "   4. Interest Rates Apy 2025\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Chunks retrieved: 4\n",
      "   Confidence: high\n",
      "   Needed retry: No\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 4: What's the overdraft fee and any waiver conditions?\n",
      "============================================================\n",
      "ðŸ¤” Thinking about: What's the overdraft fee and any waiver conditions?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: The overdraft fee is $35.00 per occurrence. However, there are several conditions under which this f...\n",
      "\n",
      "ðŸ“ Answer:\n",
      "The overdraft fee is $35.00 per occurrence. However, there are several conditions under which this fee can be waived. If the overdraft is $5.00 or less, no fee will be charged. If you bring your account back to a positive balance within 24 hours, the fee will be waived. Additionally, the fee will be waived for the first overdraft in a 12-month period if you are a customer in good standing. There is also a 24-hour grace period to deposit funds before the fee is assessed, which starts at the end of the business day when the overdraft occurs. This grace period applies to the first overdraft incident per day only.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "   1. Mobile Deposit Limits 2025\n",
      "   2. Atm Network Reimbursement 2025\n",
      "   3. Overdraft Nsf Policy 2025\n",
      "   4. Personal Checking Fees 2025\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Chunks retrieved: 4\n",
      "   Confidence: high\n",
      "   Needed retry: No\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the simple autonomous RAG agent\n",
    "test_queries = [\n",
    "    \"What's the same-day domestic wire cut-off time?\",\n",
    "    \"How many ATM fee reimbursements per month on Premier Checking?\", \n",
    "    \"What's the mobile check deposit limit for new accounts?\",\n",
    "    \"What's the overdraft fee and any waiver conditions?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing Simple Autonomous RAG Agent\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Process query - let the agent think for itself\n",
    "    response = rag_agent.process_query(query, show_thinking=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Answer:\")\n",
    "    print(response.answer)\n",
    "    \n",
    "    if response.sources:\n",
    "        print(f\"\\nðŸ“š Sources:\")\n",
    "        for j, source in enumerate(response.sources, 1):\n",
    "            print(f\"   {j}. {source}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Summary:\")\n",
    "    print(f\"   Chunks retrieved: {response.retrieved_chunks}\")\n",
    "    print(f\"   Confidence: {response.confidence}\")\n",
    "    print(f\"   Needed retry: {'Yes' if response.needed_retry else 'No'}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853236d3",
   "metadata": {},
   "source": [
    "## ðŸ”„ Demonstrating Re-retrieval Logic\n",
    "\n",
    "Let's test with a query that should trigger re-retrieval due to ambiguous terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb161d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤” Testing Autonomous Decision-Making\n",
      "Query: What are the cutoff times for money transfers?\n",
      "--------------------------------------------------\n",
      "ðŸ¤” Thinking about: What are the cutoff times for money transfers?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: For domestic wire transfers, the cut-off times for same-day processing are 3:00 PM ET on standard bu...\n",
      "\n",
      "ðŸ“ Final Answer:\n",
      "For domestic wire transfers, the cut-off times for same-day processing are 3:00 PM ET on standard business days and 2:30 PM ET on Fridays. There is no same-day processing on federal holidays. For next business day processing, the cut-off time is 5:00 PM ET on business days. \n",
      "\n",
      "For international wire transfers, the cut-off time for same-day initiation is 2:00 PM ET. \n",
      "\n",
      "For mobile deposits, the cut-off times for same-day processing are Monday-Friday before 6:00 PM ET and Saturday before 12:00 PM ET. There is no processing on Sundays. Deposits made after these cut-off times will be processed the next business day.\n",
      "\n",
      "ðŸ§  Agent Decision Process:\n",
      "   â€¢ Initial documents found: 4\n",
      "   â€¢ Agent decided to retry: No\n",
      "   â€¢ Final confidence: medium\n",
      "   âœ… Agent was satisfied with initial results\n"
     ]
    }
   ],
   "source": [
    "# Test autonomous decision-making with an ambiguous query\n",
    "ambiguous_query = \"What are the cutoff times for money transfers?\"\n",
    "\n",
    "print(\"ðŸ¤” Testing Autonomous Decision-Making\")\n",
    "print(f\"Query: {ambiguous_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = rag_agent.process_query(ambiguous_query, show_thinking=True)\n",
    "\n",
    "print(f\"\\nðŸ“ Final Answer:\")\n",
    "print(response.answer)\n",
    "\n",
    "print(f\"\\nðŸ§  Agent Decision Process:\")\n",
    "print(f\"   â€¢ Initial documents found: {response.retrieved_chunks}\")\n",
    "print(f\"   â€¢ Agent decided to retry: {'Yes' if response.needed_retry else 'No'}\")\n",
    "print(f\"   â€¢ Final confidence: {response.confidence}\")\n",
    "\n",
    "if response.needed_retry:\n",
    "    print(f\"   âœ… Agent autonomously decided more information was needed\")\n",
    "else:\n",
    "    print(f\"   âœ… Agent was satisfied with initial results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f50607",
   "metadata": {},
   "source": [
    "## ðŸ›¡ï¸ Testing Insufficient Evidence Handling\n",
    "\n",
    "Let's test with a query outside our knowledge base scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffd4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›¡ï¸ Testing Out-of-Scope Query Handling\n",
      "Query: What's the interest rate on personal loans?\n",
      "--------------------------------------------------\n",
      "ðŸ¤” Thinking about: What's the interest rate on personal loans?\n",
      "ðŸ” Found 4 documents\n",
      "ðŸ’­ Initial answer: I'm sorry, but I don't have enough information in my knowledge base to answer that question. The doc...\n",
      "ðŸ”„ LLM thinks we should search again...\n",
      "ðŸŽ¯ Trying: Current average interest rates for individual loans\n",
      "âœ… Found 6 documents with better query\n",
      "\n",
      "ðŸ“ Answer:\n",
      "I'm sorry, but I don't have enough information in my knowledge base to answer that question about the interest rate on personal loans.\n",
      "\n",
      "ðŸŽ¯ Agent Analysis:\n",
      "   â€¢ Documents found: 6\n",
      "   â€¢ Confidence level: low\n",
      "   â€¢ Tried to search again: Yes\n",
      "   âœ… Agent correctly identified insufficient information\n"
     ]
    }
   ],
   "source": [
    "# Test with a query outside knowledge base scope\n",
    "out_of_scope_query = \"What's the interest rate on personal loans?\"\n",
    "\n",
    "print(\"ðŸ›¡ï¸ Testing Out-of-Scope Query Handling\")\n",
    "print(f\"Query: {out_of_scope_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = rag_agent.process_query(out_of_scope_query, show_thinking=True)\n",
    "\n",
    "print(f\"\\nðŸ“ Answer:\")\n",
    "print(response.answer)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Agent Analysis:\")\n",
    "print(f\"   â€¢ Documents found: {response.retrieved_chunks}\")\n",
    "print(f\"   â€¢ Confidence level: {response.confidence}\")\n",
    "print(f\"   â€¢ Tried to search again: {'Yes' if response.needed_retry else 'No'}\")\n",
    "\n",
    "if \"don't have enough information\" in response.answer.lower():\n",
    "    print(f\"   âœ… Agent correctly identified insufficient information\")\n",
    "else:\n",
    "    print(f\"   â“ Agent response needs review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2ced4",
   "metadata": {},
   "source": [
    "## ðŸ“Š Query History Analysis\n",
    "\n",
    "Let's examine the agent's query processing history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcc017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Simple RAG Agent Summary\n",
      "Total queries processed: 6\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Agent Intelligence:\n",
      "   â€¢ Autonomous retry rate: 1/6 (16.7%)\n",
      "   â€¢ High confidence answers: 3/6 (50.0%)\n",
      "\n",
      "ðŸ“ˆ Performance:\n",
      "   â€¢ Average chunks per query: 4.3\n",
      "\n",
      "ðŸ’¬ Sample Interaction:\n",
      "   Q: What's the same-day domestic wire cut-off time?\n",
      "   A: The same-day processing cut-off times for domestic wire transfers are as follows: On standard busine...\n",
      "   Sources: 4 documents\n",
      "   Autonomous retry: No\n",
      "\n",
      "ðŸŽ¯ Key Advantages of Simple Autonomous RAG:\n",
      "   âœ… No complex intent parsing or keyword matching\n",
      "   âœ… LLM decides when to search again (autonomous)\n",
      "   âœ… LLM generates better search queries for retry\n",
      "   âœ… Direct OpenAI + Chroma integration\n",
      "   âœ… Minimal code, maximum intelligence\n",
      "   âœ… Agent thinks for itself, not rule-based\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Simple RAG Agent Summary\")\n",
    "print(f\"Total queries processed: {len(rag_agent.query_history)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simple analytics\n",
    "retry_count = sum(1 for r in rag_agent.query_history if r.needed_retry)\n",
    "high_confidence = sum(1 for r in rag_agent.query_history if r.confidence == \"high\")\n",
    "total_queries = len(rag_agent.query_history)\n",
    "\n",
    "if total_queries > 0:\n",
    "    print(f\"ðŸ¤– Agent Intelligence:\")\n",
    "    print(f\"   â€¢ Autonomous retry rate: {retry_count}/{total_queries} ({retry_count/total_queries*100:.1f}%)\")\n",
    "    print(f\"   â€¢ High confidence answers: {high_confidence}/{total_queries} ({high_confidence/total_queries*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Performance:\")\n",
    "    avg_chunks = sum(r.retrieved_chunks for r in rag_agent.query_history) / total_queries\n",
    "    print(f\"   â€¢ Average chunks per query: {avg_chunks:.1f}\")\n",
    "    \n",
    "    # Show a sample interaction\n",
    "    if rag_agent.query_history:\n",
    "        sample = rag_agent.query_history[0]\n",
    "        print(f\"\\nðŸ’¬ Sample Interaction:\")\n",
    "        print(f\"   Q: {sample.query}\")\n",
    "        print(f\"   A: {sample.answer[:100]}...\")\n",
    "        print(f\"   Sources: {len(sample.sources)} documents\")\n",
    "        print(f\"   Autonomous retry: {'Yes' if sample.needed_retry else 'No'}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Advantages of Simple Autonomous RAG:\")\n",
    "print(f\"   âœ… No complex intent parsing or keyword matching\")\n",
    "print(f\"   âœ… LLM decides when to search again (autonomous)\")\n",
    "print(f\"   âœ… LLM generates better search queries for retry\")\n",
    "print(f\"   âœ… Direct OpenAI + Chroma integration\")\n",
    "print(f\"   âœ… Minimal code, maximum intelligence\")\n",
    "print(f\"   âœ… Agent thinks for itself, not rule-based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1343a3",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Simple Autonomous RAG System\n",
    "\n",
    "### âœ… Core Features Demonstrated\n",
    "\n",
    "1. **Autonomous Decision-Making** âœ“\n",
    "   - LLM decides when initial retrieval is insufficient\n",
    "   - No manual intent parsing or keyword matching\n",
    "   - Agent thinks for itself about what information is needed\n",
    "\n",
    "2. **Self-Improving Search** âœ“\n",
    "   - LLM generates better search queries for retry\n",
    "   - Natural language understanding of what went wrong\n",
    "   - Automatic query expansion based on context\n",
    "\n",
    "3. **Direct Integration** âœ“\n",
    "   - OpenAI GPT-4 for reasoning and answer generation\n",
    "   - Chroma vector database for document storage and retrieval\n",
    "   - No complex framework overhead\n",
    "\n",
    "4. **Intelligent Re-Retrieval** âœ“\n",
    "   - LLM evaluates answer quality and completeness\n",
    "   - Automatically searches again with improved queries\n",
    "   - Single retry to maintain performance\n",
    "\n",
    "5. **Graceful Handling** âœ“\n",
    "   - Properly handles out-of-scope questions\n",
    "   - Admits when information is insufficient\n",
    "   - Provides confidence assessment\n",
    "\n",
    "### ðŸ§  How It Works\n",
    "\n",
    "1. **Query Processing**: User asks a question\n",
    "2. **Initial Search**: Retrieve relevant documents from Chroma\n",
    "3. **Answer Generation**: LLM creates answer from retrieved context\n",
    "4. **Quality Check**: LLM evaluates if answer is complete/helpful\n",
    "5. **Autonomous Retry**: If needed, LLM generates better search and tries again\n",
    "6. **Final Response**: Return answer with sources and confidence\n",
    "\n",
    "### ðŸš€ Key Advantages\n",
    "\n",
    "- **No Rule Engineering**: LLM handles all decision-making naturally\n",
    "- **Self-Directed**: Agent autonomously improves its search strategy\n",
    "- **Minimal Code**: Simple, maintainable implementation\n",
    "- **Maximum Intelligence**: Leverages LLM's reasoning capabilities\n",
    "- **Production Ready**: Direct OpenAI + Chroma integration\n",
    "\n",
    "### ðŸ”§ Technical Components\n",
    "\n",
    "- **Vector Store**: Chroma for semantic document search\n",
    "- **LLM**: OpenAI GPT-4 for reasoning and generation  \n",
    "- **Embeddings**: OpenAI text-embedding-ada-002\n",
    "- **Autonomy**: LLM-driven retry decisions and query improvement\n",
    "- **Simplicity**: Minimal abstraction layers, direct API calls\n",
    "\n",
    "This autonomous RAG system demonstrates how modern LLMs can manage their own retrieval process without complex rule-based logic. The agent naturally understands when it needs more information and how to search for it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
