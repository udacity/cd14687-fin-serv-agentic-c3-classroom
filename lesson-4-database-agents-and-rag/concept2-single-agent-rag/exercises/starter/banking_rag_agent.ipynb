{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312a1cd7",
   "metadata": {},
   "source": [
    "# Single Agent RAG: Banking Policy Assistant\n",
    "\n",
    "**Objective**: Build a single-agent RAG system that retrieves banking policy information from a local Chroma vector store and provides accurate answers with citations.\n",
    "\n",
    "**Key Features**:\n",
    "- ğŸ” Parse user queries to extract intent and entities\n",
    "- ğŸ“š Retrieve relevant documents from Chroma vector store\n",
    "- ğŸ¯ Generate answers with explicit citations\n",
    "- ğŸ”„ Re-retrieve if initial evidence is insufficient\n",
    "- ğŸ›¡ï¸ Handle insufficient evidence gracefully\n",
    "\n",
    "**Time**: ~15-20 minutes\n",
    "\n",
    "**Scenario**: Support a bank's customer service team with instant access to policy information for fees, limits, and procedures.\n",
    "\n",
    "## ğŸ“Š Banking Knowledge Base\n",
    "\n",
    "Our vector store contains comprehensive banking policies:\n",
    "- Personal Checking Fee Schedule (2025)\n",
    "- Domestic & International Wire Transfer Policy (2025)  \n",
    "- ATM Network & Reimbursement Rules (2025)\n",
    "- Overdraft & NSF Policy (Rev. Apr 2025)\n",
    "- Mobile Deposit Limits & Hold Periods (Consumer)\n",
    "- Interest Tiers & APY Disclosures (Savings/MMA)\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will:\n",
    "1. Build a minimal RAG system using OpenAI + Chroma\n",
    "2. Implement autonomous decision-making for retrieval\n",
    "3. Create a self-improving search mechanism\n",
    "4. Handle edge cases gracefully\n",
    "5. Understand when to retry document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5aa51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI for LLM and embeddings\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Chroma for vector storage\n",
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"ğŸ”§ Environment Setup:\")\n",
    "print(f\"   âœ… OpenAI API Key: {'âœ“ Configured' if os.getenv('OPENAI_API_KEY') else 'âŒ Missing'}\")\n",
    "print(\"   ğŸ“š Knowledge Base: Banking policy documents\") \n",
    "print(\"   ğŸ¤– Simple RAG with direct OpenAI + Chroma integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b3962",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Data Models for RAG System\n",
    "\n",
    "First, let's define a simple data structure to hold our RAG responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleRAGResponse:\n",
    "    \"\"\"Simple response from our minimal RAG agent\"\"\"\n",
    "    # YOUR CODE HERE: Define the response data structure\n",
    "    # Hint: Include query, answer, sources, retrieved_chunks, needed_retry, confidence\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ“‹ Simple data model defined for minimal RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b58cc",
   "metadata": {},
   "source": [
    "## ğŸ“š Knowledge Base Setup\n",
    "\n",
    "Now let's create functions to load banking documents into Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9168f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_banking_documents_to_chroma():\n",
    "    \"\"\"Load banking documents directly into Chroma with embeddings\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE: Initialize Chroma client\n",
    "    # Hint: Use chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    collection_name = \"banking_policies_simple\"\n",
    "    \n",
    "    # YOUR CODE HERE: Check if collection exists and create/get collection\n",
    "    # Hint: Use chroma_client.list_collections() and chroma_client.get_collection()\n",
    "    # If collection doesn't exist, create it with chroma_client.create_collection()\n",
    "    \n",
    "    # YOUR CODE HERE: Load documents from data directory\n",
    "    # Hint: \n",
    "    # 1. Check if ./data directory exists\n",
    "    # 2. Loop through .txt files in the directory\n",
    "    # 3. Read file content and split into chunks\n",
    "    # 4. Create documents, metadatas, and ids lists\n",
    "    # 5. Add documents to collection using collection.add()\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Simple text chunking function\"\"\"\n",
    "    # YOUR CODE HERE: Implement text chunking\n",
    "    # Hint: \n",
    "    # 1. Split text into words\n",
    "    # 2. Create chunks of chunk_size words with overlap\n",
    "    # 3. Return list of chunk strings\n",
    "    pass\n",
    "\n",
    "# Setup the knowledge base\n",
    "print(\"ğŸš€ Setting up simple RAG system...\")\n",
    "chroma_collection = load_banking_documents_to_chroma()\n",
    "\n",
    "if chroma_collection and chroma_collection.count() > 0:\n",
    "    print(\"âœ… Simple RAG system ready!\")\n",
    "    print(f\"   ğŸ“Š Collection: {chroma_collection.count()} document chunks\")\n",
    "else:\n",
    "    print(\"âŒ Failed to setup RAG system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13d26f",
   "metadata": {},
   "source": [
    "## ğŸ¤– Single Agent RAG System\n",
    "\n",
    "Now let's build the core RAG agent that can autonomously decide when to retry searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGAgent:\n",
    "    \"\"\"Minimal autonomous RAG agent that thinks for itself\"\"\"\n",
    "    \n",
    "    def __init__(self, chroma_collection):\n",
    "        self.collection = chroma_collection\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve_documents(self, query: str, n_results: int = 4) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant documents from Chroma\"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE: Query the Chroma collection\n",
    "        # Hint: Use self.collection.query() with query_texts and n_results\n",
    "        # Return a dictionary with documents, metadatas, distances, and count\n",
    "        pass\n",
    "    \n",
    "    def ask_llm_for_answer(self, query: str, context_docs: List[str]) -> str:\n",
    "        \"\"\"Ask the LLM to answer based on retrieved documents\"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE: Create a prompt for the LLM\n",
    "        # Hint:\n",
    "        # 1. Build context from retrieved documents\n",
    "        # 2. Create a prompt that instructs the LLM to answer based only on the documents\n",
    "        # 3. Use client.chat.completions.create() with gpt-4\n",
    "        # 4. Return the response content\n",
    "        pass\n",
    "    \n",
    "    def should_retry_retrieval(self, query: str, answer: str, retrieved_docs: List[str]) -> bool:\n",
    "        \"\"\"Let the LLM decide if we need to search again\"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE: Create a prompt asking the LLM to evaluate if retry is needed\n",
    "        # Hint:\n",
    "        # 1. Ask LLM to respond with only \"YES\" or \"NO\"\n",
    "        # 2. Explain when to say YES (not enough info, vague answer, missing details)\n",
    "        # 3. Use temperature=0 for consistent decisions\n",
    "        # 4. Return True if response is \"YES\"\n",
    "        pass\n",
    "    \n",
    "    def generate_better_search_query(self, original_query: str, previous_answer: str) -> str:\n",
    "        \"\"\"Let the LLM generate a better search query for retry\"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE: Ask LLM to generate a better search query\n",
    "        # Hint:\n",
    "        # 1. Provide original query and previous answer as context\n",
    "        # 2. Ask for different keywords, synonyms, or more specific terms\n",
    "        # 3. Keep the new query under 10 words\n",
    "        # 4. Use temperature=0.3 for some creativity\n",
    "        pass\n",
    "    \n",
    "    def assess_confidence(self, answer: str, retrieved_count: int, best_distance: float) -> str:\n",
    "        \"\"\"Simple confidence assessment\"\"\"\n",
    "        # YOUR CODE HERE: Implement confidence logic\n",
    "        # Hint: \n",
    "        # - Return \"low\" if answer mentions insufficient information\n",
    "        # - Return \"high\" if many documents retrieved with good similarity\n",
    "        # - Return \"medium\" for moderate cases\n",
    "        pass\n",
    "    \n",
    "    def process_query(self, query: str, show_thinking: bool = False) -> SimpleRAGResponse:\n",
    "        \"\"\"Main method - autonomous RAG processing\"\"\"\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"ğŸ¤” Thinking about: {query}\")\n",
    "        \n",
    "        # YOUR CODE HERE: Implement the complete RAG process\n",
    "        # Hint: Follow these steps:\n",
    "        # 1. Initial document retrieval\n",
    "        # 2. Generate initial answer from LLM\n",
    "        # 3. Ask LLM if retry is needed\n",
    "        # 4. If retry needed:\n",
    "        #    - Generate better search query\n",
    "        #    - Retrieve documents again with more results\n",
    "        #    - Generate new answer if better results found\n",
    "        # 5. Extract sources from metadata\n",
    "        # 6. Assess confidence\n",
    "        # 7. Create and return SimpleRAGResponse\n",
    "        # 8. Add to query history\n",
    "        pass\n",
    "\n",
    "# Initialize the simple RAG agent\n",
    "if chroma_collection and chroma_collection.count() > 0:\n",
    "    rag_agent = SimpleRAGAgent(chroma_collection)\n",
    "    print(\"ğŸ¤– Simple autonomous RAG agent ready!\")\n",
    "    print(\"   ğŸ§  LLM decides when to re-search\")\n",
    "    print(\"   ğŸ”„ Self-improving search queries\") \n",
    "    print(\"   ğŸ“Š No complex intent parsing needed\")\n",
    "else:\n",
    "    print(\"âŒ Cannot initialize agent - no documents in collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5439da",
   "metadata": {},
   "source": [
    "## ğŸ§ª Test the RAG System\n",
    "\n",
    "Let's test our banking RAG agent with realistic customer service questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0177f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simple autonomous RAG agent\n",
    "test_queries = [\n",
    "    \"What's the same-day domestic wire cut-off time?\",\n",
    "    \"How many ATM fee reimbursements per month on Premier Checking?\", \n",
    "    \"What's the mobile check deposit limit for new accounts?\",\n",
    "    \"What's the overdraft fee and any waiver conditions?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Simple Autonomous RAG Agent\\n\")\n",
    "\n",
    "# YOUR CODE HERE: Test the RAG agent with the test queries\n",
    "# Hint:\n",
    "# 1. Loop through test_queries\n",
    "# 2. Call rag_agent.process_query() for each query\n",
    "# 3. Display the results including answer, sources, confidence, etc.\n",
    "# 4. Show whether retry was needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45388b03",
   "metadata": {},
   "source": [
    "## ğŸ”„ Demonstrating Re-retrieval Logic\n",
    "\n",
    "Let's test with a query that should trigger re-retrieval due to ambiguous terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test autonomous decision-making with an ambiguous query\n",
    "ambiguous_query = \"What are the cutoff times for money transfers?\"\n",
    "\n",
    "print(\"ğŸ¤” Testing Autonomous Decision-Making\")\n",
    "print(f\"Query: {ambiguous_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# YOUR CODE HERE: Test the ambiguous query\n",
    "# Hint:\n",
    "# 1. Call rag_agent.process_query() with show_thinking=True\n",
    "# 2. Display the final answer and decision process\n",
    "# 3. Show whether the agent decided to retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b18e0",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ Testing Insufficient Evidence Handling\n",
    "\n",
    "Let's test with a query outside our knowledge base scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a query outside knowledge base scope\n",
    "out_of_scope_query = \"What's the interest rate on personal loans?\"\n",
    "\n",
    "print(\"ğŸ›¡ï¸ Testing Out-of-Scope Query Handling\")\n",
    "print(f\"Query: {out_of_scope_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# YOUR CODE HERE: Test the out-of-scope query\n",
    "# Hint:\n",
    "# 1. Process the query with the RAG agent\n",
    "# 2. Check if the agent correctly identifies insufficient information\n",
    "# 3. Display confidence level and retry decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dfb15",
   "metadata": {},
   "source": [
    "## ğŸ“Š Query History Analysis\n",
    "\n",
    "Let's examine the agent's query processing history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Simple RAG Agent Summary\")\n",
    "print(f\"Total queries processed: {len(rag_agent.query_history)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# YOUR CODE HERE: Analyze the agent's performance\n",
    "# Hint:\n",
    "# 1. Calculate retry rate (how often agent decided to retry)\n",
    "# 2. Calculate high confidence rate\n",
    "# 3. Show average chunks retrieved per query\n",
    "# 4. Display a sample interaction\n",
    "# 5. Highlight key advantages of the autonomous approach\n",
    "\n",
    "print(f\"\\nğŸ¯ Key Advantages of Simple Autonomous RAG:\")\n",
    "print(f\"   âœ… No complex intent parsing or keyword matching\")\n",
    "print(f\"   âœ… LLM decides when to search again (autonomous)\")\n",
    "print(f\"   âœ… LLM generates better search queries for retry\")\n",
    "print(f\"   âœ… Direct OpenAI + Chroma integration\")\n",
    "print(f\"   âœ… Minimal code, maximum intelligence\")\n",
    "print(f\"   âœ… Agent thinks for itself, not rule-based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a88e4",
   "metadata": {},
   "source": [
    "## ğŸ¯ Challenge: Implement Your RAG System\n",
    "\n",
    "### âœ… What You Need to Complete\n",
    "\n",
    "1. **Data Structure** âœï¸\n",
    "   - Complete the `SimpleRAGResponse` dataclass\n",
    "   - Include all necessary fields for tracking RAG responses\n",
    "\n",
    "2. **Document Loading** âœï¸\n",
    "   - Implement `load_banking_documents_to_chroma()`\n",
    "   - Handle Chroma collection creation and document loading\n",
    "   - Implement `split_text_into_chunks()` for text processing\n",
    "\n",
    "3. **Core RAG Methods** âœï¸\n",
    "   - Complete `retrieve_documents()` for Chroma querying\n",
    "   - Implement `ask_llm_for_answer()` with proper prompting\n",
    "   - Build `should_retry_retrieval()` for autonomous decision-making\n",
    "   - Create `generate_better_search_query()` for search improvement\n",
    "\n",
    "4. **Main Processing Logic** âœï¸\n",
    "   - Complete `process_query()` with full RAG pipeline\n",
    "   - Implement retry logic and confidence assessment\n",
    "   - Handle edge cases and error conditions\n",
    "\n",
    "5. **Testing and Analysis** âœï¸\n",
    "   - Test the agent with provided queries\n",
    "   - Analyze performance metrics\n",
    "   - Demonstrate autonomous decision-making\n",
    "\n",
    "### ğŸ§  Key Concepts to Remember\n",
    "\n",
    "- **Autonomous Decisions**: Let the LLM decide when to retry, not rules\n",
    "- **Self-Improving Search**: LLM generates better queries for retry\n",
    "- **Direct Integration**: Use OpenAI + Chroma without heavy frameworks\n",
    "- **Graceful Handling**: Admit when information is insufficient\n",
    "- **Confidence Assessment**: Evaluate answer quality objectively\n",
    "\n",
    "### ğŸš€ Success Criteria\n",
    "\n",
    "Your RAG system should:\n",
    "- âœ… Retrieve relevant documents from Chroma\n",
    "- âœ… Generate accurate answers with citations\n",
    "- âœ… Autonomously decide when to retry searches\n",
    "- âœ… Handle out-of-scope queries gracefully\n",
    "- âœ… Provide confidence assessments\n",
    "- âœ… Maintain query history for analysis\n",
    "\n",
    "**Good luck building your autonomous RAG agent! ğŸ¤–**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}