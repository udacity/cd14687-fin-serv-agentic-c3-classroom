{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a715d1",
   "metadata": {},
   "source": [
    "# Single Agent RAG: Employee Benefits Assistant\n",
    "\n",
    "This notebook demonstrates building a **single-agent RAG (Retrieval-Augmented Generation) system** that retrieves employee benefits and HR policy information from a vector store and provides accurate answers.\n",
    "\n",
    "## Key Concepts Demonstrated\n",
    "\n",
    "- **Document Retrieval**: Semantic search with Chroma vector database\n",
    "- **LLM-Powered Answers**: GPT-4 generates responses from retrieved context\n",
    "- **Autonomous Decision-Making**: Agent decides when to re-search for better information\n",
    "- **Self-Improving Queries**: LLM generates better search terms when needed\n",
    "- **Confidence Assessment**: Evaluate answer quality and completeness\n",
    "\n",
    "## Scenario\n",
    "\n",
    "An HR chatbot that helps employees get instant answers to benefits questions without searching through policy documents. The agent handles questions about health insurance, PTO, 401(k), remote work, professional development, and parental leave.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Simple & Autonomous RAG Pattern:**\n",
    "1. User asks a question\n",
    "2. System retrieves relevant policy documents\n",
    "3. LLM generates answer from retrieved context\n",
    "4. LLM evaluates if answer is complete\n",
    "5. If incomplete, LLM generates better search query and retries\n",
    "6. Return answer with sources and confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3d30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment Setup:\n",
      "   ‚úÖ OpenAI API Key: ‚úì Configured\n",
      "   üìö Knowledge Base: Employee benefits and HR policies\n",
      "   ü§ñ Autonomous RAG with OpenAI + Chroma\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI for LLM and embeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "# Chroma for vector storage\n",
    "import chromadb\n",
    "\n",
    "# Initialize OpenAI client with Vocareum endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"üîß Environment Setup:\")\n",
    "print(f\"   ‚úÖ OpenAI API Key: {'‚úì Configured' if os.getenv('OPENAI_API_KEY') else '‚ùå Missing'}\")\n",
    "print(\"   üìö Knowledge Base: Employee benefits and HR policies\") \n",
    "print(\"   ü§ñ Autonomous RAG with OpenAI + Chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac57bc",
   "metadata": {},
   "source": [
    "## Data Models for RAG System\n",
    "\n",
    "Define the structure for our RAG responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e412e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Data model defined for RAG responses\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class RAGResponse:\n",
    "    \"\"\"Response from our autonomous RAG agent\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "    retrieved_chunks: int\n",
    "    needed_retry: bool\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "\n",
    "print(\"üìã Data model defined for RAG responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b015fda",
   "metadata": {},
   "source": [
    "## Knowledge Base Setup\n",
    "\n",
    "Load HR policy documents into Chroma vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f16467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setting up RAG system...\n",
      "üìù Creating new collection: hr_policies\n",
      "   üìÑ Loaded: parental_leave_policy_2025.txt (3 chunks)\n",
      "   üìÑ Loaded: health_insurance_benefits_2025.txt (1 chunks)\n",
      "   üìÑ Loaded: professional_development_policy_2025.txt (3 chunks)\n",
      "   üìÑ Loaded: paid_time_off_policy_2025.txt (1 chunks)\n",
      "   üìÑ Loaded: remote_work_policy_2025.txt (2 chunks)\n",
      "   üìÑ Loaded: retirement_401k_plan_2025.txt (2 chunks)\n",
      "üéâ Added 12 document chunks to Chroma\n",
      "\n",
      "‚úÖ RAG system ready!\n",
      "   üìä Collection: 12 document chunks\n",
      "   üîç Ready to answer HR policy questions\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks for better retrieval\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_hr_policies_to_chroma():\n",
    "    \"\"\"Load HR policy documents into Chroma with embeddings\"\"\"\n",
    "    \n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    collection_name = \"hr_policies\"\n",
    "    \n",
    "    # Check if collection already exists\n",
    "    try:\n",
    "        existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"üìö Using existing collection: {collection_name}\")\n",
    "            collection = chroma_client.get_collection(collection_name)\n",
    "            print(f\"   üìä Documents in collection: {collection.count()}\")\n",
    "            return collection\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(f\"üìù Creating new collection: {collection_name}\")\n",
    "    \n",
    "    # Create new collection\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"description\": \"Employee benefits and HR policies\"}\n",
    "    )\n",
    "    \n",
    "    # Load documents from data directory\n",
    "    data_dir = \"./data\"\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"‚ùå Data directory '{data_dir}' not found!\")\n",
    "        return collection\n",
    "    \n",
    "    doc_id = 0\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Split content into chunks\n",
    "                chunks = split_text_into_chunks(content, chunk_size=500, overlap=50)\n",
    "                \n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    documents.append(chunk)\n",
    "                    metadatas.append({\n",
    "                        'filename': filename,\n",
    "                        'doc_title': filename.replace('_', ' ').replace('.txt', '').replace('2025', '').title().strip(),\n",
    "                        'chunk_id': f\"{filename}_{i}\"\n",
    "                    })\n",
    "                    ids.append(f\"doc_{doc_id}\")\n",
    "                    doc_id += 1\n",
    "                \n",
    "                print(f\"   üìÑ Loaded: {filename} ({len(chunks)} chunks)\")\n",
    "    \n",
    "    if documents:\n",
    "        # Add documents to collection (Chroma auto-generates embeddings)\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"üéâ Added {len(documents)} document chunks to Chroma\")\n",
    "    else:\n",
    "        print(\"‚ùå No documents found to add\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "# Setup the knowledge base\n",
    "print(\"üöÄ Setting up RAG system...\")\n",
    "chroma_collection = load_hr_policies_to_chroma()\n",
    "\n",
    "if chroma_collection.count() > 0:\n",
    "    print(\"\\n‚úÖ RAG system ready!\")\n",
    "    print(f\"   üìä Collection: {chroma_collection.count()} document chunks\")\n",
    "    print(f\"   üîç Ready to answer HR policy questions\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to setup RAG system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac30a08",
   "metadata": {},
   "source": [
    "## Build the Autonomous RAG Agent\n",
    "\n",
    "This agent orchestrates the entire RAG pipeline with autonomous decision-making:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796f770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Employee Benefits RAG Agent initialized!\n",
      "   üß† Autonomous decision-making enabled\n",
      "   üîÑ Self-improving search queries\n",
      "   üìä Confidence assessment active\n"
     ]
    }
   ],
   "source": [
    "class EmployeeBenefitsRAGAgent:\n",
    "    \"\"\"Autonomous RAG agent for employee benefits questions\"\"\"\n",
    "    \n",
    "    def __init__(self, chroma_collection):\n",
    "        self.collection = chroma_collection\n",
    "        self.query_history = []\n",
    "    \n",
    "    def retrieve_documents(self, query: str, n_results: int = 4) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant documents from Chroma\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'documents': results['documents'][0] if results['documents'] else [],\n",
    "            'metadatas': results['metadatas'][0] if results['metadatas'] else [],\n",
    "            'distances': results['distances'][0] if results['distances'] else [],\n",
    "            'count': len(results['documents'][0]) if results['documents'] else 0\n",
    "        }\n",
    "    \n",
    "    def generate_answer(self, query: str, context_docs: List[str]) -> str:\n",
    "        \"\"\"Generate answer using LLM with retrieved context\"\"\"\n",
    "        \n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful HR assistant helping employees understand their benefits and policies. Answer based ONLY on the provided documents.\n",
    "\n",
    "RETRIEVED POLICY DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "EMPLOYEE QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer based ONLY on information in the documents above\n",
    "- If documents contain the answer, provide clear, helpful response\n",
    "- If documents lack information, say \"I don't have enough information in the policy documents to answer that question\"\n",
    "- Include specific details like amounts, time periods, eligibility requirements\n",
    "- Be conversational but professional\n",
    "- For complex topics, break down the answer into clear points\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def should_retry(self, query: str, answer: str, retrieved_docs: List[str]) -> bool:\n",
    "        \"\"\"Let LLM decide if we need to search again\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate if a RAG system should search again for better information.\n",
    "\n",
    "QUESTION: {query}\n",
    "ANSWER: {answer}\n",
    "DOCUMENTS FOUND: {len(retrieved_docs)}\n",
    "\n",
    "Respond with ONLY \"YES\" or \"NO\".\n",
    "\n",
    "Say \"YES\" if:\n",
    "- Answer says not enough information\n",
    "- Answer is vague or incomplete\n",
    "- Question asks for specifics but answer doesn't provide them\n",
    "\n",
    "Say \"NO\" if:\n",
    "- Answer provides specific, helpful information\n",
    "- Answer appropriately explains what's unavailable\n",
    "\n",
    "DECISION:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip().upper() == \"YES\"\n",
    "    \n",
    "    def improve_query(self, original_query: str, previous_answer: str) -> str:\n",
    "        \"\"\"Generate better search query for retry\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Generate a better search query for HR policy documents.\n",
    "\n",
    "ORIGINAL: {original_query}\n",
    "PREVIOUS ANSWER: {previous_answer}\n",
    "\n",
    "The search didn't find good information. Generate a NEW search query that might find better results.\n",
    "\n",
    "Tips:\n",
    "- Use different keywords or synonyms\n",
    "- Try broader or more specific terms\n",
    "- Focus on key HR/benefits concepts\n",
    "- Keep under 10 words\n",
    "\n",
    "NEW QUERY:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def assess_confidence(self, answer: str, retrieved_count: int, best_distance: float) -> str:\n",
    "        \"\"\"Assess confidence in the answer\"\"\"\n",
    "        if \"don't have enough information\" in answer.lower():\n",
    "            return \"low\"\n",
    "        elif retrieved_count >= 3 and best_distance < 0.8:\n",
    "            return \"high\"\n",
    "        elif retrieved_count >= 2:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def process_query(self, query: str, show_thinking: bool = False) -> RAGResponse:\n",
    "        \"\"\"Main method - autonomous RAG processing with retry logic\"\"\"\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"ü§î Processing: {query}\")\n",
    "        \n",
    "        # Step 1: Initial retrieval\n",
    "        results = self.retrieve_documents(query, n_results=4)\n",
    "        docs = results['documents']\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"üìö Found {results['count']} relevant documents\")\n",
    "        \n",
    "        # Step 2: Generate initial answer\n",
    "        answer = self.generate_answer(query, docs)\n",
    "        \n",
    "        if show_thinking:\n",
    "            print(f\"üí≠ Generated initial answer\")\n",
    "        \n",
    "        # Step 3: Let LLM decide if retry needed\n",
    "        needs_retry = False\n",
    "        total_chunks = results['count']\n",
    "        \n",
    "        if results['count'] > 0:\n",
    "            needs_retry = self.should_retry(query, answer, docs)\n",
    "        \n",
    "        # Step 4: Retry if needed\n",
    "        if needs_retry:\n",
    "            if show_thinking:\n",
    "                print(f\"üîÑ Agent decided to search again...\")\n",
    "            \n",
    "            better_query = self.improve_query(query, answer)\n",
    "            if show_thinking:\n",
    "                print(f\"üéØ New search: {better_query}\")\n",
    "            \n",
    "            retry_results = self.retrieve_documents(better_query, n_results=6)\n",
    "            \n",
    "            if retry_results['count'] > results['count']:\n",
    "                results = retry_results\n",
    "                docs = results['documents']\n",
    "                answer = self.generate_answer(query, docs)\n",
    "                total_chunks = retry_results['count']\n",
    "                \n",
    "                if show_thinking:\n",
    "                    print(f\"‚úÖ Found {retry_results['count']} documents with improved query\")\n",
    "        \n",
    "        # Step 5: Extract sources\n",
    "        sources = []\n",
    "        if results['metadatas']:\n",
    "            sources = list(set([meta.get('doc_title', 'Unknown') for meta in results['metadatas']]))\n",
    "        \n",
    "        # Step 6: Assess confidence\n",
    "        best_distance = min(results['distances']) if results['distances'] else 1.0\n",
    "        confidence = self.assess_confidence(answer, results['count'], best_distance)\n",
    "        \n",
    "        # Create response\n",
    "        response = RAGResponse(\n",
    "            query=query,\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            retrieved_chunks=total_chunks,\n",
    "            needed_retry=needs_retry,\n",
    "            confidence=confidence\n",
    "        )\n",
    "        \n",
    "        self.query_history.append(response)\n",
    "        return response\n",
    "\n",
    "# Initialize the agent\n",
    "if chroma_collection.count() > 0:\n",
    "    rag_agent = EmployeeBenefitsRAGAgent(chroma_collection)\n",
    "    print(\"ü§ñ Employee Benefits RAG Agent initialized!\")\n",
    "    print(\"   üß† Autonomous decision-making enabled\")\n",
    "    print(\"   üîÑ Self-improving search queries\")\n",
    "    print(\"   üìä Confidence assessment active\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize agent - no documents loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c61f",
   "metadata": {},
   "source": [
    "## Test the RAG System\n",
    "\n",
    "Let's test with realistic employee benefits questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8125aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Employee Benefits RAG Agent\n",
      "\n",
      "ü§î Processing: How much does the company match for 401k contributions?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "======================================================================\n",
      "‚ùì Question: How much does the company match for 401k contributions?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "The company matches 401(k) contributions based on the following formula:\n",
      "\n",
      "- The company will match 100% of the first 3% of your salary that you contribute.\n",
      "- Then, the company will match 50% of the next 2% of your salary that you contribute.\n",
      "\n",
      "This means that if you contribute 5% or more of your salary, the maximum company contribution will be 4% of your salary. For example, if you contribute 1% of your salary, the company will match 1%, making a total of 2%. If you contribute 3%, the company will match 3%, making a total of 6%. If you contribute 5% or more, the company will match 4%, so if you contribute 10%, the total contribution will be 14%.\n",
      "\n",
      "The company match is contributed with each paycheck and is subject to a vesting schedule. The vesting schedule is graded, meaning that the percentage of the company match that you own increases over time. After 1 year, you are 0% vested, after 2 years you are 25% vested, after 3 years you are 50% vested, after 4 years you are 75% vested, and after 5 or more years you are 100% vested. Your own contributions are always 100% vested immediately.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Health Insurance Benefits\n",
      "   ‚Ä¢ Parental Leave Policy\n",
      "   ‚Ä¢ Retirement 401K Plan\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 4\n",
      "   ‚Ä¢ Confidence: medium\n",
      "   ‚Ä¢ Needed retry: No\n",
      "\n",
      "\n",
      "ü§î Processing: What are the PTO accrual rates for someone with 4 years of service?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "======================================================================\n",
      "‚ùì Question: What are the PTO accrual rates for someone with 4 years of service?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "For an employee with 4 years of service, the Paid Time Off (PTO) accrual rates are as follows:\n",
      "\n",
      "- If you are a full-time employee, you will accrue 20 days per year, which is approximately 1.67 days per month.\n",
      "- If you are a part-time employee working 20 or more hours per week, you will accrue 10 days per year.\n",
      "\n",
      "Please note that these accrual rates are in addition to the 10 paid company holidays per year and separate sick leave of 5 days per year.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Parental Leave Policy\n",
      "   ‚Ä¢ Paid Time Off Policy\n",
      "   ‚Ä¢ Professional Development Policy\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 4\n",
      "   ‚Ä¢ Confidence: medium\n",
      "   ‚Ä¢ Needed retry: No\n",
      "\n",
      "\n",
      "ü§î Processing: What's covered under the Premium health insurance plan?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "======================================================================\n",
      "‚ùì Question: What's covered under the Premium health insurance plan?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "The Premium health insurance plan offers comprehensive coverage with the following details:\n",
      "\n",
      "1. Monthly Premium: $150 for an individual employee and $450 for a family.\n",
      "2. Annual Deductible: $500 for an individual and $1,500 for a family.\n",
      "3. Out-of-pocket Maximum: $3,000 for an individual and $9,000 for a family.\n",
      "4. Copay: $20 for primary care and $40 for specialist visits.\n",
      "5. Prescription Coverage: $10 for generic drugs, $30 for brand-name drugs, and $50 for specialty medications.\n",
      "6. Coverage: The plan covers 90% of costs after the deductible has been met.\n",
      "\n",
      "In addition to these, the plan also includes wellness benefits such as 100% coverage for annual physical exams, preventive screenings, and flu shots. Health coaching is available at no cost. If you have a gym membership, you can get reimbursed up to $50 per month.\n",
      "\n",
      "Dental and vision benefits are included in the plan at no additional cost. This includes two dental cleanings per year and one comprehensive eye exam per year, along with a $200 allowance for glasses or contacts annually.\n",
      "\n",
      "Please note that you can enroll in this plan during the open enrollment period, which is from November 1-30 annually, or within a 30-day window from a qualifying life event such as marriage, divorce, birth or adoption of a child, loss of other coverage, or a change in employment status.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Health Insurance Benefits\n",
      "   ‚Ä¢ Parental Leave Policy\n",
      "   ‚Ä¢ Retirement 401K Plan\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 4\n",
      "   ‚Ä¢ Confidence: medium\n",
      "   ‚Ä¢ Needed retry: No\n",
      "\n",
      "\n",
      "ü§î Processing: How much paid parental leave do primary caregivers get?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "======================================================================\n",
      "‚ùì Question: How much paid parental leave do primary caregivers get?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "Primary caregivers are eligible for 16 weeks, or 112 days, of fully paid parental leave. This applies to both birth mothers and primary adoptive parents. The leave can be taken continuously or split within 12 months from the birth or placement of the child. There is no minimum service requirement for this benefit, and it applies to all regular full-time and part-time employees who work 20 or more hours per week.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Parental Leave Policy\n",
      "   ‚Ä¢ Paid Time Off Policy\n",
      "   ‚Ä¢ Retirement 401K Plan\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 4\n",
      "   ‚Ä¢ Confidence: high\n",
      "   ‚Ä¢ Needed retry: No\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def display_response(response: RAGResponse):\n",
    "    \"\"\"Pretty print RAG response\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚ùì Question: {response.query}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüí¨ Answer:\\n{response.answer}\")\n",
    "    \n",
    "    if response.sources:\n",
    "        print(f\"\\nüìö Sources:\")\n",
    "        for source in response.sources:\n",
    "            print(f\"   ‚Ä¢ {source}\")\n",
    "    \n",
    "    print(f\"\\nüìä Metadata:\")\n",
    "    print(f\"   ‚Ä¢ Chunks retrieved: {response.retrieved_chunks}\")\n",
    "    print(f\"   ‚Ä¢ Confidence: {response.confidence}\")\n",
    "    print(f\"   ‚Ä¢ Needed retry: {'Yes' if response.needed_retry else 'No'}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Test queries\n",
    "test_questions = [\n",
    "    \"How much does the company match for 401k contributions?\",\n",
    "    \"What are the PTO accrual rates for someone with 4 years of service?\",\n",
    "    \"What's covered under the Premium health insurance plan?\",\n",
    "    \"How much paid parental leave do primary caregivers get?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Employee Benefits RAG Agent\\n\")\n",
    "\n",
    "for question in test_questions:\n",
    "    response = rag_agent.process_query(question, show_thinking=True)\n",
    "    display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b622a",
   "metadata": {},
   "source": [
    "## Test Autonomous Re-Retrieval\n",
    "\n",
    "Test with an ambiguous query that should trigger the agent's autonomous retry logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8222956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Autonomous Decision-Making\n",
      "Query: What are the remote work requirements?\n",
      "----------------------------------------------------------------------\n",
      "ü§î Processing: What are the remote work requirements?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "======================================================================\n",
      "‚ùì Question: What are the remote work requirements?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "The remote work requirements are outlined in the Remote Work and Flexible Schedule Policy. Here are the key points:\n",
      "\n",
      "1. **Eligibility**: Your role should not require on-site presence and you should have demonstrated self-management and productivity. You should have consistent performance ratings of \"Meets Expectations\" or above. You also need a reliable internet connection (minimum 50 Mbps download) and a dedicated workspace free from distractions. \n",
      "\n",
      "2. **Work Hours and Availability**: You should be available for meetings and collaboration during core hours, which are 10AM-3PM local time. You're expected to respond to messages within 30 minutes. You can complete your 8-hour workday between 6AM-10PM, but you need to adjust start/end times with manager approval and record actual hours in the time tracking system.\n",
      "\n",
      "3. **Equipment and Technology**: The company provides equipment such as a laptop, monitor, keyboard, mouse, and headset for video calls. You will also receive a one-time stipend of $500 for a desk and ergonomic chair, and a monthly stipend of $75 for internet reimbursement. You're required to use a VPN for a secure connection and other specified software.\n",
      "\n",
      "4. **Workspace Requirements**: You should have a dedicated workspace separate from living areas, with proper lighting and an ergonomic setup. The company will reimburse up to $300/month for co-working spaces, but these must be secure, private workspaces at pre-approved locations.\n",
      "\n",
      "5. **Performance and Accountability**: You're expected to meet all deadlines and deliverables, maintain the same performance standards as office work, complete weekly status updates, and respond to communications promptly.\n",
      "\n",
      "6. **Expenses and Reimbursement**: You can be reimbursed for internet service up to $75/month, mobile phone up to $50/month, printer supplies up to $25/month, and office supplies up to $30/month.\n",
      "\n",
      "7. **Return to Office Protocol**: The company reserves the right to require office presence with a minimum 30 days notice for policy changes and 24-hour notice for emergency situations.\n",
      "\n",
      "8. **Policy Violations**: Violations of the policy, such as consistent unavailability during core hours or missing deadlines, can lead to warnings and potentially revocation of remote privileges.\n",
      "\n",
      "9. **Special Circumstances**: Temporary remote work may be approved for medical reasons, caregiving responsibilities, weather emergencies, or home emergencies.\n",
      "\n",
      "Please let me know if you need more information on any of these points.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Remote Work Policy\n",
      "   ‚Ä¢ Professional Development Policy\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 4\n",
      "   ‚Ä¢ Confidence: high\n",
      "   ‚Ä¢ Needed retry: No\n",
      "\n",
      "\n",
      "üß† Agent Analysis:\n",
      "   ‚úÖ Agent was satisfied with initial retrieval\n"
     ]
    }
   ],
   "source": [
    "# Ambiguous query that may need better search terms\n",
    "ambiguous_query = \"What are the remote work requirements?\"\n",
    "\n",
    "print(\"üîç Testing Autonomous Decision-Making\")\n",
    "print(f\"Query: {ambiguous_query}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "response = rag_agent.process_query(ambiguous_query, show_thinking=True)\n",
    "display_response(response)\n",
    "\n",
    "print(\"üß† Agent Analysis:\")\n",
    "if response.needed_retry:\n",
    "    print(\"   ‚úÖ Agent autonomously decided to search again for better results\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Agent was satisfied with initial retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d392e",
   "metadata": {},
   "source": [
    "## Test Out-of-Scope Handling\n",
    "\n",
    "Test how the agent handles questions outside the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087c9e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Testing Out-of-Scope Question Handling\n",
      "Query: What's the company's revenue for last quarter?\n",
      "----------------------------------------------------------------------\n",
      "ü§î Processing: What's the company's revenue for last quarter?\n",
      "üìö Found 4 relevant documents\n",
      "üí≠ Generated initial answer\n",
      "üîÑ Agent decided to search again...\n",
      "üéØ New search: \"Company's HR policies on employee compensation and benefits\"\n",
      "‚úÖ Found 6 documents with improved query\n",
      "======================================================================\n",
      "‚ùì Question: What's the company's revenue for last quarter?\n",
      "======================================================================\n",
      "\n",
      "üí¨ Answer:\n",
      "I don't have enough information in the policy documents to answer that question.\n",
      "\n",
      "üìö Sources:\n",
      "   ‚Ä¢ Paid Time Off Policy\n",
      "   ‚Ä¢ Remote Work Policy\n",
      "   ‚Ä¢ Parental Leave Policy\n",
      "   ‚Ä¢ Retirement 401K Plan\n",
      "   ‚Ä¢ Professional Development Policy\n",
      "\n",
      "üìä Metadata:\n",
      "   ‚Ä¢ Chunks retrieved: 6\n",
      "   ‚Ä¢ Confidence: low\n",
      "   ‚Ä¢ Needed retry: Yes\n",
      "\n",
      "\n",
      "‚úÖ Agent correctly identified insufficient information\n"
     ]
    }
   ],
   "source": [
    "# Question outside our HR policies\n",
    "out_of_scope_query = \"What's the company's revenue for last quarter?\"\n",
    "\n",
    "print(\"üõ°Ô∏è Testing Out-of-Scope Question Handling\")\n",
    "print(f\"Query: {out_of_scope_query}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "response = rag_agent.process_query(out_of_scope_query, show_thinking=True)\n",
    "display_response(response)\n",
    "\n",
    "if \"don't have enough information\" in response.answer.lower():\n",
    "    print(\"‚úÖ Agent correctly identified insufficient information\")\n",
    "else:\n",
    "    print(\"‚ùì Review agent's handling of out-of-scope questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894635d",
   "metadata": {},
   "source": [
    "## Query History Analysis\n",
    "\n",
    "Analyze the agent's performance across all queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a6f99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RAG Agent Performance Summary\n",
      "======================================================================\n",
      "Total queries processed: 6\n",
      "\n",
      "ü§ñ Autonomous Behavior:\n",
      "   ‚Ä¢ Retry rate: 1/6 (16.7%)\n",
      "   ‚Ä¢ High confidence answers: 2/6 (33.3%)\n",
      "\n",
      "üìà Retrieval Metrics:\n",
      "   ‚Ä¢ Average chunks per query: 4.3\n",
      "\n",
      "üéØ Confidence Distribution:\n",
      "   ‚Ä¢ High: 2 (33.3%)\n",
      "   ‚Ä¢ Low: 1 (16.7%)\n",
      "   ‚Ä¢ Medium: 3 (50.0%)\n",
      "\n",
      "‚ú® Key Features Demonstrated:\n",
      "   ‚úÖ Autonomous retry decisions (LLM-driven)\n",
      "   ‚úÖ Self-improving search queries\n",
      "   ‚úÖ Confidence assessment\n",
      "   ‚úÖ Source citation\n",
      "   ‚úÖ Graceful handling of missing information\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä RAG Agent Performance Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total queries processed: {len(rag_agent.query_history)}\\n\")\n",
    "\n",
    "if len(rag_agent.query_history) > 0:\n",
    "    # Calculate metrics\n",
    "    retry_count = sum(1 for r in rag_agent.query_history if r.needed_retry)\n",
    "    high_confidence = sum(1 for r in rag_agent.query_history if r.confidence == \"high\")\n",
    "    total = len(rag_agent.query_history)\n",
    "    \n",
    "    print(f\"ü§ñ Autonomous Behavior:\")\n",
    "    print(f\"   ‚Ä¢ Retry rate: {retry_count}/{total} ({retry_count/total*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ High confidence answers: {high_confidence}/{total} ({high_confidence/total*100:.1f}%)\")\n",
    "    \n",
    "    avg_chunks = sum(r.retrieved_chunks for r in rag_agent.query_history) / total\n",
    "    print(f\"\\nüìà Retrieval Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Average chunks per query: {avg_chunks:.1f}\")\n",
    "    \n",
    "    # Show confidence distribution\n",
    "    confidence_dist = {}\n",
    "    for r in rag_agent.query_history:\n",
    "        confidence_dist[r.confidence] = confidence_dist.get(r.confidence, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüéØ Confidence Distribution:\")\n",
    "    for conf, count in sorted(confidence_dist.items()):\n",
    "        print(f\"   ‚Ä¢ {conf.capitalize()}: {count} ({count/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚ú® Key Features Demonstrated:\")\n",
    "print(f\"   ‚úÖ Autonomous retry decisions (LLM-driven)\")\n",
    "print(f\"   ‚úÖ Self-improving search queries\")\n",
    "print(f\"   ‚úÖ Confidence assessment\")\n",
    "print(f\"   ‚úÖ Source citation\")\n",
    "print(f\"   ‚úÖ Graceful handling of missing information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d444e208",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ **Core RAG Concepts Demonstrated**\n",
    "\n",
    "1. **Document Retrieval** - Semantic search with vector embeddings\n",
    "2. **Context-Aware Generation** - LLM generates answers from retrieved documents only\n",
    "3. **Autonomous Decision-Making** - Agent decides when to search again\n",
    "4. **Query Improvement** - LLM generates better search terms automatically\n",
    "5. **Source Attribution** - Tracks which documents were used\n",
    "6. **Confidence Assessment** - Evaluates answer quality\n",
    "\n",
    "### üèóÔ∏è **Architecture Pattern**\n",
    "\n",
    "**Simple Autonomous RAG:**\n",
    "- Direct OpenAI + Chroma integration\n",
    "- No complex frameworks needed\n",
    "- LLM handles all decision-making\n",
    "- Self-improving through retry logic\n",
    "- Minimal code, maximum intelligence\n",
    "\n",
    "### üöÄ **Production Considerations**\n",
    "\n",
    "- **Chunking Strategy**: Overlap ensures context isn't lost at boundaries\n",
    "- **Retrieval Count**: Start with 4, increase to 6 on retry for better coverage\n",
    "- **Temperature**: Low (0.1) for factual answers, higher (0.3) for query generation\n",
    "- **Prompt Engineering**: Clear instructions ensure LLM stays grounded in documents\n",
    "- **Error Handling**: Gracefully admits when information is insufficient\n",
    "\n",
    "### üí° **Applications**\n",
    "\n",
    "This pattern extends to:\n",
    "- **Customer Support**: Product documentation, troubleshooting guides\n",
    "- **Legal/Compliance**: Policy documents, regulatory requirements\n",
    "- **Healthcare**: Medical protocols, patient information\n",
    "- **Education**: Course materials, study guides\n",
    "- **Finance**: Product information, regulatory filings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
