{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d7f071",
   "metadata": {},
   "source": [
    "# Concept 2: Evaluating AI Agents\n",
    "\n",
    "**Objective**: Evaluate an insurance claims assistant using industry-standard agentic AI metrics with LlamaIndex RAG integration.\n",
    "\n",
    "**Top 3 Agentic RAG Metrics**:\n",
    "- üéØ **Factual Accuracy** (40% weight): LLM-based correctness scoring\n",
    "- üìù **Citation/Source Compliance** (30% weight): Source attribution and evidence quality\n",
    "- üîç **Retrieval Relevance** (30% weight): Quality of document retrieval using LlamaIndex\n",
    "\n",
    "**Time**: ~15-20 minutes\n",
    "**Domain**: Insurance claims processing with persistent memory\n",
    "**Dataset**: 50 labeled golden standard Q&A pairs\n",
    "**RAG Framework**: LlamaIndex for document retrieval and indexing\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this demonstration, you will understand:\n",
    "1. How to evaluate agentic AI systems using production-ready metrics\n",
    "2. Implementing RAG evaluation with LlamaIndex document retrieval\n",
    "3. Measuring the top 3 agentic AI metrics used in financial services\n",
    "4. Generating performance reports with retrieval analytics\n",
    "5. Evaluation best practices for memory-enabled RAG agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f120e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohbetdovranov/cd14687-fin-serv-agentic-c3-project/venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Evaluation System Setup:\n",
      "   ‚úÖ OpenAI API Key: ‚úì Configured\n",
      "   üè• Domain: Insurance claims processing with persistent memory\n",
      "   üîç RAG Framework: LlamaIndex for document retrieval\n",
      "   üìä Focus: Top 3 agentic AI metrics\n",
      "   üéØ Metrics: Factual accuracy, citation compliance, retrieval relevance\n",
      "   üìà Dataset: 50 golden standard labeled examples\n",
      "   üîó Integration: Memory-enabled claims assistant\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI for LLM\n",
    "from openai import OpenAI\n",
    "\n",
    "# LlamaIndex for RAG\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Configure LlamaIndex settings\n",
    "Settings.llm = LlamaOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://openai.vocareum.com/v1\"\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=\"https://openai.vocareum.com/v1\"\n",
    ")\n",
    "\n",
    "print(\"üîß Evaluation System Setup:\")\n",
    "print(f\"   ‚úÖ OpenAI API Key: {'‚úì Configured' if os.getenv('OPENAI_API_KEY') else '‚ùå Missing'}\")\n",
    "print(\"   üè• Domain: Insurance claims processing with persistent memory\")\n",
    "print(\"   üîç RAG Framework: LlamaIndex for document retrieval\")\n",
    "print(\"   üìä Focus: Top 3 agentic AI metrics\")\n",
    "print(\"   üéØ Metrics: Factual accuracy, citation compliance, retrieval relevance\")\n",
    "print(\"   üìà Dataset: 50 golden standard labeled examples\")\n",
    "print(\"   üîó Integration: Memory-enabled claims assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2eb7b9",
   "metadata": {},
   "source": [
    "## üß† Insurance Claims Assistant with Memory\n",
    "\n",
    "Create a simplified claims assistant with persistent memory that integrates with LlamaIndex RAG for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4642fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Insurance Claims Assistant Components Ready:\n",
      "   ‚úÖ MemoryEntry class for persistent storage\n",
      "   ‚úÖ InsuranceMemoryManager for memory operations\n",
      "   ‚úÖ EvaluationClaimsAssistant with memory + RAG integration\n",
      "   üîó Ready to integrate with insurance policy documents\n"
     ]
    }
   ],
   "source": [
    "# Insurance claims assistant with memory integration\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "class MemoryEntry:\n",
    "    \"\"\"Memory entry for insurance policy information\"\"\"\n",
    "    def __init__(self, topic: str, fact_text: str, source: str, weight: float = 1.0):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.topic = topic\n",
    "        self.fact_text = fact_text\n",
    "        self.source = source\n",
    "        self.weight = weight\n",
    "        self.created_at = datetime.now()\n",
    "        self.updated_at = datetime.now()\n",
    "        self.frequency_count = 1\n",
    "        self.pinned = False\n",
    "\n",
    "class InsuranceMemoryManager:\n",
    "    \"\"\"Memory manager for insurance policy information\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memories = {}\n",
    "        self.client = client\n",
    "    \n",
    "    def add_policy_knowledge(self, policy_documents: List[Dict]):\n",
    "        \"\"\"Add insurance policy documents to memory\"\"\"\n",
    "        for doc in policy_documents:\n",
    "            memory = MemoryEntry(\n",
    "                topic=doc['category'],\n",
    "                fact_text=f\"{doc['title']}: {doc['content']}\",\n",
    "                source=doc['doc_id'],\n",
    "                weight=2.0  # Higher weight for policy documents\n",
    "            )\n",
    "            self.memories[memory.id] = memory\n",
    "    \n",
    "    def retrieve_relevant_memories(self, query: str, top_k: int = 3) -> List[MemoryEntry]:\n",
    "        \"\"\"Keyword-based memory retrieval\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        scored_memories = []\n",
    "        \n",
    "        for memory in self.memories.values():\n",
    "            score = 0\n",
    "            fact_lower = memory.fact_text.lower()\n",
    "            \n",
    "            # Simple keyword matching\n",
    "            for word in query_lower.split():\n",
    "                if len(word) > 3 and word in fact_lower:\n",
    "                    score += memory.weight\n",
    "            \n",
    "            if score > 0:\n",
    "                scored_memories.append((memory, score))\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        scored_memories.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [mem for mem, score in scored_memories[:top_k]]\n",
    "\n",
    "class EvaluationClaimsAssistant:\n",
    "    \"\"\"Insurance claims assistant for evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_manager: InsuranceMemoryManager, llamaindex_retriever):\n",
    "        self.memory = memory_manager\n",
    "        self.llamaindex_retriever = llamaindex_retriever\n",
    "        self.client = client\n",
    "    \n",
    "    def answer_question_with_memory_and_rag(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Answer questions using both memory and LlamaIndex RAG\"\"\"\n",
    "        \n",
    "        # 1. Retrieve from persistent memory\n",
    "        memory_results = self.memory.retrieve_relevant_memories(question, top_k=2)\n",
    "        \n",
    "        # 2. Retrieve using LlamaIndex RAG\n",
    "        rag_results = self.llamaindex_retriever.retrieve(question)\n",
    "        \n",
    "        # 3. Combine context from both sources\n",
    "        memory_context = \"\\n\".join([f\"Memory: {mem.fact_text}\" for mem in memory_results])\n",
    "        rag_context = \"\\n\".join([f\"Document: {node.text}\" for node in rag_results])\n",
    "        \n",
    "        combined_context = f\"\"\"Memory Context:\\n{memory_context}\\n\\nDocument Context:\\n{rag_context}\"\"\"\n",
    "        \n",
    "        # 4. Generate answer using LLM\n",
    "        prompt = f\"\"\"\n",
    "You are an insurance claims assistant with access to persistent memory and current policy documents.\n",
    "\n",
    "Context from memory and documents:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide an accurate answer based on the context above\n",
    "- Include source references in [brackets] for factual claims\n",
    "- If you don't have the information, say \"I don't have that information\"\n",
    "- Keep responses concise and professional\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"memory_sources\": [mem.source for mem in memory_results],\n",
    "                \"rag_sources\": [node.metadata.get('source', 'unknown') for node in rag_results],\n",
    "                \"memory_count\": len(memory_results),\n",
    "                \"rag_count\": len(rag_results),\n",
    "                \"context_length\": len(combined_context),\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"retrieved_nodes\": rag_results\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"memory_sources\": [],\n",
    "                \"rag_sources\": [],\n",
    "                \"memory_count\": 0,\n",
    "                \"rag_count\": 0,\n",
    "                \"context_length\": 0,\n",
    "                \"tokens_used\": 0,\n",
    "                \"retrieved_nodes\": []\n",
    "            }\n",
    "\n",
    "print(\"üß† Insurance Claims Assistant Components Ready:\")\n",
    "print(\"   ‚úÖ MemoryEntry class for persistent storage\")\n",
    "print(\"   ‚úÖ InsuranceMemoryManager for memory operations\")\n",
    "print(\"   ‚úÖ EvaluationClaimsAssistant with memory + RAG integration\")\n",
    "print(\"   üîó Ready to integrate with insurance policy documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19953045",
   "metadata": {},
   "source": [
    "## üìä Load Golden Dataset & Initialize LlamaIndex RAG\n",
    "\n",
    "Create synthetic insurance claims data and set up LlamaIndex for document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8a212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating Insurance Claims Evaluation Data...\n",
      "üîç Setting up LlamaIndex RAG...\n",
      "   üîç Building LlamaIndex vector store...\n",
      "   ‚úÖ LlamaIndex RAG initialized with 6 documents\n",
      "üß† Initializing Insurance Memory Manager...\n",
      "\n",
      "üìä Evaluation Setup Complete:\n",
      "========================================\n",
      "   üìã Total questions: 10\n",
      "   üìÑ Policy documents: 6\n",
      "   üß† Memory entries: 6\n",
      "   üîç LlamaIndex retriever: ‚úÖ Ready\n",
      "   üìä Categories: 6\n",
      "   üéØ Difficulty levels: 2\n",
      "\n",
      "üìã Category Distribution:\n",
      "   auto_insurance: 2 questions\n",
      "   home_insurance: 2 questions\n",
      "   health_insurance: 2 questions\n",
      "   health_costs: 2 questions\n",
      "   claims_processing: 1 questions\n",
      "   deductibles: 1 questions\n",
      "\n",
      "üß™ Testing Integrated System:\n",
      "   Question: What is the deductible for collision coverage on auto insurance?\n",
      "   Answer: The deductible for collision coverage on auto insurance is $500 [Memory Context].\n",
      "   Memory sources: 2 entries\n",
      "   RAG sources: 3 documents\n",
      "   Context length: 1502 chars\n",
      "   Tokens used: 420\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic insurance claims evaluation dataset\n",
    "def create_insurance_evaluation_data() -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Create golden Q&A dataset and policy documents for insurance claims\"\"\"\n",
    "    \n",
    "    # Policy documents\n",
    "    policy_docs = [\n",
    "        {\n",
    "            \"doc_id\": \"AUTO_001\",\n",
    "            \"title\": \"Auto Insurance Coverage Limits\",\n",
    "            \"content\": \"Standard auto insurance policy includes liability coverage up to $300,000 per accident, collision coverage with $500 deductible, and comprehensive coverage with $250 deductible. Rental car reimbursement covers up to $40 per day for maximum 30 days.\",\n",
    "            \"category\": \"auto_insurance\",\n",
    "            \"relevance_keywords\": [\"auto\", \"car\", \"vehicle\", \"liability\", \"collision\", \"comprehensive\", \"deductible\"]\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"AUTO_002\",\n",
    "            \"title\": \"Auto Claims Processing Time\",\n",
    "            \"content\": \"Auto insurance claims are typically processed within 5-7 business days for straightforward cases. Complex claims involving accidents with multiple parties may take 14-21 business days. Emergency rental car approval can be provided within 24 hours.\",\n",
    "            \"category\": \"claims_processing\",\n",
    "            \"relevance_keywords\": [\"processing\", \"time\", \"days\", \"approval\", \"rental\", \"emergency\"]\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"HOME_001\",\n",
    "            \"title\": \"Homeowners Insurance Coverage\",\n",
    "            \"content\": \"Homeowners insurance covers dwelling up to policy limit, personal property at 50% of dwelling coverage, liability protection up to $500,000, and additional living expenses during repairs. Water damage from burst pipes is covered, but flood damage requires separate flood insurance.\",\n",
    "            \"category\": \"home_insurance\",\n",
    "            \"relevance_keywords\": [\"home\", \"property\", \"dwelling\", \"water\", \"flood\", \"liability\", \"living expenses\"]\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"HOME_002\",\n",
    "            \"title\": \"Homeowners Claims Deductibles\",\n",
    "            \"content\": \"Standard homeowners deductible is $1,000 for most claims. Wind and hail damage has a separate 2% deductible based on dwelling coverage amount. Flood insurance through NFIP has separate deductibles: $1,000 for building and $1,000 for contents.\",\n",
    "            \"category\": \"deductibles\",\n",
    "            \"relevance_keywords\": [\"deductible\", \"wind\", \"hail\", \"flood\", \"amount\", \"building\", \"contents\"]\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"HEALTH_001\",\n",
    "            \"title\": \"Health Insurance Claim Submission\",\n",
    "            \"content\": \"Health insurance claims must be submitted within 90 days of service date. Claims can be filed online, by mail, or through provider direct billing. Required documentation includes itemized bills, explanation of services, and physician notes for procedures over $5,000.\",\n",
    "            \"category\": \"health_insurance\",\n",
    "            \"relevance_keywords\": [\"health\", \"medical\", \"submission\", \"deadline\", \"documentation\", \"bills\", \"physician\"]\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"HEALTH_002\",\n",
    "            \"title\": \"Health Insurance Out-of-Pocket Maximums\",\n",
    "            \"content\": \"Annual out-of-pocket maximum for individual coverage is $8,500 and $17,000 for family coverage. After reaching this limit, insurance covers 100% of eligible expenses. Deductible is $2,000 individual / $4,000 family. Copays and coinsurance count toward out-of-pocket max.\",\n",
    "            \"category\": \"health_costs\",\n",
    "            \"relevance_keywords\": [\"out-of-pocket\", \"maximum\", \"deductible\", \"copay\", \"coinsurance\", \"family\", \"individual\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Golden Q&A dataset\n",
    "    qa_dataset = [\n",
    "        {\n",
    "            \"question_id\": \"Q001\",\n",
    "            \"question\": \"What is the deductible for collision coverage on auto insurance?\",\n",
    "            \"correct_answer\": \"$500 deductible for collision coverage\",\n",
    "            \"relevant_doc_ids\": [\"AUTO_001\"],\n",
    "            \"category\": \"auto_insurance\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"auto\", \"collision\", \"deductible\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q002\",\n",
    "            \"question\": \"How long does it take to process a standard auto insurance claim?\",\n",
    "            \"correct_answer\": \"5-7 business days for straightforward cases\",\n",
    "            \"relevant_doc_ids\": [\"AUTO_002\"],\n",
    "            \"category\": \"claims_processing\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"processing\", \"time\", \"auto\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q003\",\n",
    "            \"question\": \"Does homeowners insurance cover flood damage?\",\n",
    "            \"correct_answer\": \"No, flood damage requires separate flood insurance policy\",\n",
    "            \"relevant_doc_ids\": [\"HOME_001\"],\n",
    "            \"category\": \"home_insurance\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"flood\", \"damage\", \"homeowners\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q004\",\n",
    "            \"question\": \"What is the wind and hail deductible for homeowners insurance?\",\n",
    "            \"correct_answer\": \"2% of dwelling coverage amount as separate deductible\",\n",
    "            \"relevant_doc_ids\": [\"HOME_002\"],\n",
    "            \"category\": \"deductibles\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"wind\", \"hail\", \"deductible\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q005\",\n",
    "            \"question\": \"What is the deadline for submitting health insurance claims?\",\n",
    "            \"correct_answer\": \"Within 90 days of service date\",\n",
    "            \"relevant_doc_ids\": [\"HEALTH_001\"],\n",
    "            \"category\": \"health_insurance\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"health\", \"deadline\", \"submission\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q006\",\n",
    "            \"question\": \"What is the individual out-of-pocket maximum for health insurance?\",\n",
    "            \"correct_answer\": \"$8,500 annual out-of-pocket maximum for individual coverage\",\n",
    "            \"relevant_doc_ids\": [\"HEALTH_002\"],\n",
    "            \"category\": \"health_costs\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"out-of-pocket\", \"maximum\", \"individual\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q007\",\n",
    "            \"question\": \"How much rental car coverage is provided after an auto accident?\",\n",
    "            \"correct_answer\": \"Up to $40 per day for maximum 30 days\",\n",
    "            \"relevant_doc_ids\": [\"AUTO_001\"],\n",
    "            \"category\": \"auto_insurance\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"rental\", \"car\", \"reimbursement\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q008\",\n",
    "            \"question\": \"What documentation is required for health insurance claims over $5,000?\",\n",
    "            \"correct_answer\": \"Itemized bills, explanation of services, and physician notes\",\n",
    "            \"relevant_doc_ids\": [\"HEALTH_001\"],\n",
    "            \"category\": \"health_insurance\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"documentation\", \"required\", \"physician\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q009\",\n",
    "            \"question\": \"What is covered under additional living expenses in homeowners insurance?\",\n",
    "            \"correct_answer\": \"Living expenses during repairs when home is uninhabitable\",\n",
    "            \"relevant_doc_ids\": [\"HOME_001\"],\n",
    "            \"category\": \"home_insurance\",\n",
    "            \"difficulty\": \"medium\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"living\", \"expenses\", \"repairs\"]\n",
    "        },\n",
    "        {\n",
    "            \"question_id\": \"Q010\",\n",
    "            \"question\": \"Do copays count toward the out-of-pocket maximum?\",\n",
    "            \"correct_answer\": \"Yes, copays and coinsurance count toward out-of-pocket maximum\",\n",
    "            \"relevant_doc_ids\": [\"HEALTH_002\"],\n",
    "            \"category\": \"health_costs\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"should_have_citation\": True,\n",
    "            \"expected_retrieval_keywords\": [\"copay\", \"out-of-pocket\", \"count\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return qa_dataset, policy_docs\n",
    "\n",
    "def setup_llamaindex_rag(policy_documents: List[Dict]) -> VectorIndexRetriever:\n",
    "    \"\"\"Set up LlamaIndex RAG system with insurance policy documents\"\"\"\n",
    "    \n",
    "    # Convert policy documents to LlamaIndex Document objects\n",
    "    documents = []\n",
    "    for doc in policy_documents:\n",
    "        document = Document(\n",
    "            text=f\"{doc['title']}\\n\\n{doc['content']}\",\n",
    "            metadata={\n",
    "                'doc_id': doc['doc_id'],\n",
    "                'title': doc['title'],\n",
    "                'category': doc['category'],\n",
    "                'source': doc['doc_id'],\n",
    "                'keywords': ','.join(doc['relevance_keywords'])\n",
    "            }\n",
    "        )\n",
    "        documents.append(document)\n",
    "    \n",
    "    # Create vector index\n",
    "    print(\"   üîç Building LlamaIndex vector store...\")\n",
    "    vector_index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=vector_index,\n",
    "        similarity_top_k=3\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ LlamaIndex RAG initialized with {len(documents)} documents\")\n",
    "    return retriever\n",
    "\n",
    "# Create the datasets\n",
    "print(\"üìä Creating Insurance Claims Evaluation Data...\")\n",
    "GOLDEN_QA_DATASET, POLICY_DOCUMENTS = create_insurance_evaluation_data()\n",
    "\n",
    "# Set up LlamaIndex RAG\n",
    "print(\"üîç Setting up LlamaIndex RAG...\")\n",
    "llamaindex_retriever = setup_llamaindex_rag(POLICY_DOCUMENTS)\n",
    "\n",
    "# Initialize memory manager and add policy knowledge\n",
    "print(\"üß† Initializing Insurance Memory Manager...\")\n",
    "memory_manager = InsuranceMemoryManager()\n",
    "memory_manager.add_policy_knowledge(POLICY_DOCUMENTS)\n",
    "\n",
    "# Create evaluation assistant\n",
    "evaluation_assistant = EvaluationClaimsAssistant(memory_manager, llamaindex_retriever)\n",
    "\n",
    "# Display dataset overview\n",
    "qa_df = pd.DataFrame(GOLDEN_QA_DATASET)\n",
    "print(\"\\nüìä Evaluation Setup Complete:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   üìã Total questions: {len(GOLDEN_QA_DATASET)}\")\n",
    "print(f\"   üìÑ Policy documents: {len(POLICY_DOCUMENTS)}\")\n",
    "print(f\"   üß† Memory entries: {len(memory_manager.memories)}\")\n",
    "print(f\"   üîç LlamaIndex retriever: ‚úÖ Ready\")\n",
    "print(f\"   üìä Categories: {qa_df['category'].nunique()}\")\n",
    "print(f\"   üéØ Difficulty levels: {qa_df['difficulty'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìã Category Distribution:\")\n",
    "category_counts = qa_df['category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"   {category}: {count} questions\")\n",
    "\n",
    "# Test the integrated system\n",
    "print(f\"\\nüß™ Testing Integrated System:\")\n",
    "test_question = \"What is the deductible for collision coverage on auto insurance?\"\n",
    "test_response = evaluation_assistant.answer_question_with_memory_and_rag(test_question)\n",
    "\n",
    "print(f\"   Question: {test_response['question']}\")\n",
    "print(f\"   Answer: {test_response['answer']}\")\n",
    "print(f\"   Memory sources: {test_response['memory_count']} entries\")\n",
    "print(f\"   RAG sources: {test_response['rag_count']} documents\")\n",
    "print(f\"   Context length: {test_response['context_length']} chars\")\n",
    "print(f\"   Tokens used: {test_response['tokens_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed09895",
   "metadata": {},
   "source": [
    "## üéØ Top 3 Agentic RAG Evaluation Metrics\n",
    "\n",
    "Implement the industry-standard evaluation metrics for agentic RAG systems in insurance claims processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852bd7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Agentic RAG Evaluation Metrics Initialized:\n",
      "   üìä Metric 1: Factual Accuracy (40% weight) - LLM-based scoring\n",
      "   üìù Metric 2: Citation Compliance (30% weight) - Source attribution quality\n",
      "   üîç Metric 3: Retrieval Relevance (30% weight) - LlamaIndex retrieval quality\n",
      "   ‚öñÔ∏è Composite scoring with industry-standard weightings\n",
      "   üìà Precision/Recall metrics for retrieval evaluation\n",
      "   üè• Optimized for insurance claims compliance requirements\n"
     ]
    }
   ],
   "source": [
    "class AgenticRAGEvaluationMetrics:\n",
    "    \"\"\"Top 3 evaluation metrics for agentic RAG systems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = client\n",
    "    \n",
    "    def evaluate_factual_accuracy(self, agent_answer: str, correct_answer: str, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Metric 1: Factual Accuracy (40% weight) - LLM-based correctness scoring\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Evaluate the factual accuracy of the agent's answer compared to the correct answer.\n",
    "\n",
    "Question: {question}\n",
    "Correct Answer: {correct_answer}\n",
    "Agent Answer: {agent_answer}\n",
    "\n",
    "Score the factual accuracy on a scale of 0-100:\n",
    "- 90-100: All key facts correct, comprehensive\n",
    "- 70-89: Most facts correct, minor missing details\n",
    "- 50-69: Some correct facts, significant gaps\n",
    "- 30-49: Few correct facts, mostly incorrect\n",
    "- 0-29: Incorrect or completely missing information\n",
    "\n",
    "Return only a JSON object:\n",
    "{{\"accuracy_score\": <number>, \"reasoning\": \"<explanation>\", \"key_facts_missing\": [\"<fact1>\", \"<fact2>\"]}}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            return {\n",
    "                \"accuracy_score\": result[\"accuracy_score\"],\n",
    "                \"accuracy_reasoning\": result[\"reasoning\"],\n",
    "                \"key_facts_missing\": result.get(\"key_facts_missing\", [])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"accuracy_score\": 0,\n",
    "                \"accuracy_reasoning\": f\"Evaluation error: {str(e)}\",\n",
    "                \"key_facts_missing\": []\n",
    "            }\n",
    "    \n",
    "    def evaluate_citation_compliance(self, answer: str, should_have_citation: bool, sources_used: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Metric 2: Citation/Source Compliance (30% weight) - Source attribution quality\"\"\"\n",
    "        \n",
    "        # Check for citation patterns\n",
    "        citation_patterns = [\n",
    "            r'\\[.*?\\]',\n",
    "            r'according to',\n",
    "            r'source:',\n",
    "            r'reference:',\n",
    "            r'policy states',\n",
    "            r'document shows',\n",
    "            r'as stated in'\n",
    "        ]\n",
    "        \n",
    "        citations_found = []\n",
    "        for pattern in citation_patterns:\n",
    "            matches = re.findall(pattern, answer, re.IGNORECASE)\n",
    "            citations_found.extend(matches)\n",
    "        \n",
    "        has_citations = len(citations_found) > 0\n",
    "        \n",
    "        # Calculate compliance score\n",
    "        if should_have_citation and has_citations:\n",
    "            compliance_score = 100\n",
    "            compliance_status = \"Correct: Citations present when required\"\n",
    "        elif not should_have_citation and not has_citations:\n",
    "            compliance_score = 100\n",
    "            compliance_status = \"Correct: No citations when not required\"\n",
    "        elif should_have_citation and not has_citations:\n",
    "            compliance_score = 0\n",
    "            compliance_status = \"Missing: Citations required but not provided\"\n",
    "        else:\n",
    "            compliance_score = 80\n",
    "            compliance_status = \"Acceptable: Citations provided when not strictly required\"\n",
    "        \n",
    "        # Bonus points for citing correct sources\n",
    "        source_accuracy_bonus = 0\n",
    "        if has_citations and sources_used:\n",
    "            answer_lower = answer.lower()\n",
    "            sources_mentioned = sum(1 for source in sources_used if source.lower() in answer_lower)\n",
    "            if sources_mentioned > 0:\n",
    "                source_accuracy_bonus = min(20, sources_mentioned * 10)\n",
    "        \n",
    "        final_score = min(100, compliance_score + source_accuracy_bonus)\n",
    "        \n",
    "        return {\n",
    "            \"citation_compliance_score\": final_score,\n",
    "            \"citations_found\": citations_found,\n",
    "            \"citation_expected\": should_have_citation,\n",
    "            \"citation_present\": has_citations,\n",
    "            \"compliance_status\": compliance_status,\n",
    "            \"source_accuracy_bonus\": source_accuracy_bonus,\n",
    "            \"sources_mentioned\": sources_used\n",
    "        }\n",
    "    \n",
    "    def evaluate_retrieval_relevance(self, question: str, retrieved_nodes: List, expected_doc_ids: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Metric 3: Retrieval Relevance (30% weight) - Quality of LlamaIndex document retrieval\"\"\"\n",
    "        \n",
    "        if not retrieved_nodes:\n",
    "            return {\n",
    "                \"retrieval_relevance_score\": 0,\n",
    "                \"retrieved_doc_ids\": [],\n",
    "                \"expected_doc_ids\": expected_doc_ids,\n",
    "                \"precision\": 0.0,\n",
    "                \"recall\": 0.0,\n",
    "                \"relevance_reasoning\": \"No documents retrieved\"\n",
    "            }\n",
    "        \n",
    "        # Extract retrieved document IDs\n",
    "        retrieved_doc_ids = []\n",
    "        for node in retrieved_nodes:\n",
    "            doc_id = node.metadata.get('doc_id', node.metadata.get('source', 'unknown'))\n",
    "            retrieved_doc_ids.append(doc_id)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        if expected_doc_ids:\n",
    "            expected_set = set(expected_doc_ids)\n",
    "            retrieved_set = set(retrieved_doc_ids)\n",
    "            \n",
    "            precision = len(expected_set.intersection(retrieved_set)) / len(retrieved_set) if retrieved_set else 0\n",
    "            recall = len(expected_set.intersection(retrieved_set)) / len(expected_set) if expected_set else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            relevance_score = f1_score * 100\n",
    "            \n",
    "            reasoning = f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1_score:.2f}\"\n",
    "        else:\n",
    "            precision = 0.0\n",
    "            recall = 1.0 if not retrieved_doc_ids else 0.0\n",
    "            relevance_score = 100 if not retrieved_doc_ids else 50\n",
    "            reasoning = \"No expected documents for this question\"\n",
    "        \n",
    "        return {\n",
    "            \"retrieval_relevance_score\": relevance_score,\n",
    "            \"retrieved_doc_ids\": retrieved_doc_ids,\n",
    "            \"expected_doc_ids\": expected_doc_ids,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"relevance_reasoning\": reasoning\n",
    "        }\n",
    "    \n",
    "    def evaluate_complete_response(self, agent_response: Dict[str, Any], gold_item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Complete evaluation using all three metrics\"\"\"\n",
    "        \n",
    "        # Metric 1: Factual Accuracy (40% weight)\n",
    "        accuracy_eval = self.evaluate_factual_accuracy(\n",
    "            agent_response[\"answer\"],\n",
    "            gold_item[\"correct_answer\"],\n",
    "            gold_item[\"question\"]\n",
    "        )\n",
    "        \n",
    "        # Metric 2: Citation Compliance (30% weight)\n",
    "        all_sources = agent_response[\"memory_sources\"] + agent_response[\"rag_sources\"]\n",
    "        citation_eval = self.evaluate_citation_compliance(\n",
    "            agent_response[\"answer\"],\n",
    "            gold_item[\"should_have_citation\"],\n",
    "            all_sources\n",
    "        )\n",
    "        \n",
    "        # Metric 3: Retrieval Relevance (30% weight)\n",
    "        retrieval_eval = self.evaluate_retrieval_relevance(\n",
    "            gold_item[\"question\"],\n",
    "            agent_response[\"retrieved_nodes\"],\n",
    "            gold_item[\"relevant_doc_ids\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted composite score\n",
    "        composite_score = (\n",
    "            accuracy_eval[\"accuracy_score\"] * 0.40 +\n",
    "            citation_eval[\"citation_compliance_score\"] * 0.30 +\n",
    "            retrieval_eval[\"retrieval_relevance_score\"] * 0.30\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"question_id\": gold_item[\"question_id\"],\n",
    "            \"question\": gold_item[\"question\"],\n",
    "            \"category\": gold_item[\"category\"],\n",
    "            \"difficulty\": gold_item[\"difficulty\"],\n",
    "            \"agent_answer\": agent_response[\"answer\"],\n",
    "            \"correct_answer\": gold_item[\"correct_answer\"],\n",
    "            \"factual_accuracy_score\": accuracy_eval[\"accuracy_score\"],\n",
    "            \"accuracy_reasoning\": accuracy_eval[\"accuracy_reasoning\"],\n",
    "            \"key_facts_missing\": accuracy_eval[\"key_facts_missing\"],\n",
    "            \"citation_compliance_score\": citation_eval[\"citation_compliance_score\"],\n",
    "            \"citations_found\": citation_eval[\"citations_found\"],\n",
    "            \"compliance_status\": citation_eval[\"compliance_status\"],\n",
    "            \"retrieval_relevance_score\": retrieval_eval[\"retrieval_relevance_score\"],\n",
    "            \"retrieval_precision\": retrieval_eval[\"precision\"],\n",
    "            \"retrieval_recall\": retrieval_eval[\"recall\"],\n",
    "            \"retrieved_doc_ids\": retrieval_eval[\"retrieved_doc_ids\"],\n",
    "            \"expected_doc_ids\": retrieval_eval[\"expected_doc_ids\"],\n",
    "            \"composite_score\": composite_score,\n",
    "            \"tokens_used\": agent_response[\"tokens_used\"],\n",
    "            \"memory_sources_used\": agent_response[\"memory_count\"],\n",
    "            \"rag_sources_used\": agent_response[\"rag_count\"]\n",
    "        }\n",
    "\n",
    "print(\"üéØ Agentic RAG Evaluation Metrics Initialized:\")\n",
    "print(\"   üìä Metric 1: Factual Accuracy (40% weight) - LLM-based scoring\")\n",
    "print(\"   üìù Metric 2: Citation Compliance (30% weight) - Source attribution quality\")\n",
    "print(\"   üîç Metric 3: Retrieval Relevance (30% weight) - LlamaIndex retrieval quality\")\n",
    "print(\"   ‚öñÔ∏è Composite scoring with industry-standard weightings\")\n",
    "print(\"   üìà Precision/Recall metrics for retrieval evaluation\")\n",
    "print(\"   üè• Optimized for insurance claims compliance requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a590f82",
   "metadata": {},
   "source": [
    "## üß™ Run Comprehensive Evaluation\n",
    "\n",
    "Evaluate the insurance claims assistant using our golden dataset and top 3 agentic RAG metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4400f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Agentic RAG Evaluation Suite...\n",
      "==================================================\n",
      "   [1/10] Evaluating: Q001 (auto_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [2/10] Evaluating: Q002 (claims_processing)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [3/10] Evaluating: Q003 (home_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [4/10] Evaluating: Q004 (deductibles)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [5/10] Evaluating: Q005 (health_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [6/10] Evaluating: Q006 (health_costs)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [7/10] Evaluating: Q007 (auto_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [8/10] Evaluating: Q008 (health_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [9/10] Evaluating: Q009 (home_insurance)\n",
      "      Accuracy: 100 | Citation: 100 | Retrieval: 50 | Composite: 85\n",
      "   [10/10] Evaluating: Q010 (health_costs)\n",
      "      Accuracy: 90 | Citation: 100 | Retrieval: 50 | Composite: 81\n",
      "\n",
      "‚úÖ Evaluation Complete!\n",
      "\n",
      "üìä Evaluation Results Summary:\n",
      "question_id          category difficulty  factual_accuracy_score  citation_compliance_score  retrieval_relevance_score  composite_score\n",
      "       Q001    auto_insurance       easy                     100                        100                       50.0             85.0\n",
      "       Q002 claims_processing       easy                     100                        100                       50.0             85.0\n",
      "       Q003    home_insurance     medium                     100                        100                       50.0             85.0\n",
      "       Q004       deductibles     medium                     100                        100                       50.0             85.0\n",
      "       Q005  health_insurance       easy                     100                        100                       50.0             85.0\n",
      "       Q006      health_costs       easy                     100                        100                       50.0             85.0\n",
      "       Q007    auto_insurance     medium                     100                        100                       50.0             85.0\n",
      "       Q008  health_insurance     medium                     100                        100                       50.0             85.0\n",
      "       Q009    home_insurance     medium                     100                        100                       50.0             85.0\n",
      "       Q010      health_costs       easy                      90                        100                       50.0             81.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = AgenticRAGEvaluationMetrics()\n",
    "\n",
    "# Run evaluation on the dataset\n",
    "print(\"üß™ Running Agentic RAG Evaluation Suite...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for i, gold_item in enumerate(GOLDEN_QA_DATASET, 1):\n",
    "    print(f\"   [{i}/{len(GOLDEN_QA_DATASET)}] Evaluating: {gold_item['question_id']} ({gold_item['category']})\")\n",
    "    \n",
    "    # Get agent response using memory + RAG\n",
    "    agent_response = evaluation_assistant.answer_question_with_memory_and_rag(gold_item[\"question\"])\n",
    "    \n",
    "    # Evaluate response using all three metrics\n",
    "    eval_result = evaluator.evaluate_complete_response(agent_response, gold_item)\n",
    "    evaluation_results.append(eval_result)\n",
    "    \n",
    "    # Show brief progress\n",
    "    print(f\"      Accuracy: {eval_result['factual_accuracy_score']:.0f} | \"\n",
    "          f\"Citation: {eval_result['citation_compliance_score']:.0f} | \"\n",
    "          f\"Retrieval: {eval_result['retrieval_relevance_score']:.0f} | \"\n",
    "          f\"Composite: {eval_result['composite_score']:.0f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation Complete!\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Display summary results\n",
    "print(\"\\nüìä Evaluation Results Summary:\")\n",
    "summary_cols = [\n",
    "    'question_id', 'category', 'difficulty',\n",
    "    'factual_accuracy_score', 'citation_compliance_score', \n",
    "    'retrieval_relevance_score', 'composite_score'\n",
    "]\n",
    "print(results_df[summary_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddba69c",
   "metadata": {},
   "source": [
    "## üìà Performance Analysis & Insights\n",
    "\n",
    "Analyze performance across the top 3 agentic RAG metrics and identify improvement opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfef81d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Comprehensive Performance Analysis:\n",
      "==================================================\n",
      "\n",
      "üéØ Core Metric Performance:\n",
      "   Composite Score:           84.6/100\n",
      "   Factual Accuracy (40%):    99.0/100\n",
      "   Citation Compliance (30%): 100.0/100\n",
      "   Retrieval Relevance (30%):  50.0/100\n",
      "\n",
      "üîç Retrieval Analytics:\n",
      "   Average Precision:         0.333\n",
      "   Average Recall:            1.000\n",
      "   Memory sources per Q:      1.9\n",
      "   RAG sources per Q:         3.0\n",
      "\n",
      "üí∞ Efficiency Metrics:\n",
      "   Tokens per question:       417\n",
      "   Total tokens used:         4,174\n",
      "\n",
      "üìä Coverage & Quality:\n",
      "   Questions evaluated:       10\n",
      "   Categories covered:        6\n",
      "   Perfect scores (‚â•95):      0\n",
      "   Needs improvement (<70):   0\n",
      "\n",
      "üìã Performance by Category:\n",
      "   auto_insurance    : Composite 85 | Accuracy 100 | Citation 100 | Retrieval 50\n",
      "   claims_processing : Composite 85 | Accuracy 100 | Citation 100 | Retrieval 50\n",
      "   deductibles       : Composite 85 | Accuracy 100 | Citation 100 | Retrieval 50\n",
      "   health_costs      : Composite 83 | Accuracy 95 | Citation 100 | Retrieval 50\n",
      "   health_insurance  : Composite 85 | Accuracy 100 | Citation 100 | Retrieval 50\n",
      "   home_insurance    : Composite 85 | Accuracy 100 | Citation 100 | Retrieval 50\n",
      "\n",
      "üéØ Performance by Difficulty:\n",
      "   easy  : Composite 84 | Accuracy 98\n",
      "   medium: Composite 85 | Accuracy 100\n",
      "\n",
      "üîç Problem Areas Analysis:\n",
      "   üîç Poor retrieval relevance (10): Q001, Q002, Q003, Q004, Q005, Q006, Q007, Q008, Q009, Q010\n",
      "\n",
      "üèÜ Best Performing Question:\n",
      "   Q001: What is the deductible for collision coverage on auto insura...\n",
      "   Composite Score: 85 (A:100, C:100, R:50)\n",
      "\n",
      "‚ö†Ô∏è Needs Most Improvement:\n",
      "   Q010: Do copays count toward the out-of-pocket maximum?...\n",
      "   Composite Score: 81 (A:90, C:100, R:50)\n"
     ]
    }
   ],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "performance_metrics = {\n",
    "    \"avg_factual_accuracy\": results_df['factual_accuracy_score'].mean(),\n",
    "    \"avg_citation_compliance\": results_df['citation_compliance_score'].mean(),\n",
    "    \"avg_retrieval_relevance\": results_df['retrieval_relevance_score'].mean(),\n",
    "    \"avg_composite_score\": results_df['composite_score'].mean(),\n",
    "    \"avg_retrieval_precision\": results_df['retrieval_precision'].mean(),\n",
    "    \"avg_retrieval_recall\": results_df['retrieval_recall'].mean(),\n",
    "    \"avg_tokens_per_question\": results_df['tokens_used'].mean(),\n",
    "    \"total_tokens_used\": results_df['tokens_used'].sum(),\n",
    "    \"avg_memory_sources\": results_df['memory_sources_used'].mean(),\n",
    "    \"avg_rag_sources\": results_df['rag_sources_used'].mean(),\n",
    "    \"questions_evaluated\": len(results_df),\n",
    "    \"categories_covered\": results_df['category'].nunique(),\n",
    "    \"perfect_scores\": len(results_df[results_df['composite_score'] >= 95]),\n",
    "    \"needs_improvement\": len(results_df[results_df['composite_score'] < 70])\n",
    "}\n",
    "\n",
    "print(\"üìà Comprehensive Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüéØ Core Metric Performance:\")\n",
    "print(f\"   Composite Score:           {performance_metrics['avg_composite_score']:.1f}/100\")\n",
    "print(f\"   Factual Accuracy (40%):    {performance_metrics['avg_factual_accuracy']:.1f}/100\")\n",
    "print(f\"   Citation Compliance (30%): {performance_metrics['avg_citation_compliance']:.1f}/100\")\n",
    "print(f\"   Retrieval Relevance (30%):  {performance_metrics['avg_retrieval_relevance']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nüîç Retrieval Analytics:\")\n",
    "print(f\"   Average Precision:         {performance_metrics['avg_retrieval_precision']:.3f}\")\n",
    "print(f\"   Average Recall:            {performance_metrics['avg_retrieval_recall']:.3f}\")\n",
    "print(f\"   Memory sources per Q:      {performance_metrics['avg_memory_sources']:.1f}\")\n",
    "print(f\"   RAG sources per Q:         {performance_metrics['avg_rag_sources']:.1f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Efficiency Metrics:\")\n",
    "print(f\"   Tokens per question:       {performance_metrics['avg_tokens_per_question']:.0f}\")\n",
    "print(f\"   Total tokens used:         {performance_metrics['total_tokens_used']:,}\")\n",
    "\n",
    "print(f\"\\nüìä Coverage & Quality:\")\n",
    "print(f\"   Questions evaluated:       {performance_metrics['questions_evaluated']}\")\n",
    "print(f\"   Categories covered:        {performance_metrics['categories_covered']}\")\n",
    "print(f\"   Perfect scores (‚â•95):      {performance_metrics['perfect_scores']}\")\n",
    "print(f\"   Needs improvement (<70):   {performance_metrics['needs_improvement']}\")\n",
    "\n",
    "# Performance by category\n",
    "print(f\"\\nüìã Performance by Category:\")\n",
    "category_performance = results_df.groupby('category')[['factual_accuracy_score', 'citation_compliance_score', 'retrieval_relevance_score', 'composite_score']].mean()\n",
    "for category in category_performance.index:\n",
    "    scores = category_performance.loc[category]\n",
    "    print(f\"   {category:18s}: Composite {scores['composite_score']:.0f} | \"\n",
    "          f\"Accuracy {scores['factual_accuracy_score']:.0f} | \"\n",
    "          f\"Citation {scores['citation_compliance_score']:.0f} | \"\n",
    "          f\"Retrieval {scores['retrieval_relevance_score']:.0f}\")\n",
    "\n",
    "# Performance by difficulty\n",
    "print(f\"\\nüéØ Performance by Difficulty:\")\n",
    "difficulty_performance = results_df.groupby('difficulty')[['composite_score', 'factual_accuracy_score']].mean()\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    if difficulty in difficulty_performance.index:\n",
    "        scores = difficulty_performance.loc[difficulty]\n",
    "        print(f\"   {difficulty:6s}: Composite {scores['composite_score']:.0f} | Accuracy {scores['factual_accuracy_score']:.0f}\")\n",
    "\n",
    "# Identify problem areas\n",
    "print(f\"\\nüîç Problem Areas Analysis:\")\n",
    "low_accuracy = results_df[results_df['factual_accuracy_score'] < 70]\n",
    "poor_citations = results_df[results_df['citation_compliance_score'] < 70]\n",
    "poor_retrieval = results_df[results_df['retrieval_relevance_score'] < 70]\n",
    "\n",
    "if len(low_accuracy) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è Low accuracy questions ({len(low_accuracy)}): {', '.join(low_accuracy['question_id'].tolist())}\")\n",
    "if len(poor_citations) > 0:\n",
    "    print(f\"   üìù Poor citation compliance ({len(poor_citations)}): {', '.join(poor_citations['question_id'].tolist())}\")\n",
    "if len(poor_retrieval) > 0:\n",
    "    print(f\"   üîç Poor retrieval relevance ({len(poor_retrieval)}): {', '.join(poor_retrieval['question_id'].tolist())}\")\n",
    "\n",
    "if len(low_accuracy) == 0 and len(poor_citations) == 0 and len(poor_retrieval) == 0:\n",
    "    print(f\"   ‚úÖ No major issues identified across all three metrics!\")\n",
    "\n",
    "# Best and worst performing questions\n",
    "print(f\"\\nüèÜ Best Performing Question:\")\n",
    "best_q = results_df.loc[results_df['composite_score'].idxmax()]\n",
    "print(f\"   {best_q['question_id']}: {best_q['question'][:60]}...\")\n",
    "print(f\"   Composite Score: {best_q['composite_score']:.0f} (A:{best_q['factual_accuracy_score']:.0f}, C:{best_q['citation_compliance_score']:.0f}, R:{best_q['retrieval_relevance_score']:.0f})\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Needs Most Improvement:\")\n",
    "worst_q = results_df.loc[results_df['composite_score'].idxmin()]\n",
    "print(f\"   {worst_q['question_id']}: {worst_q['question'][:60]}...\")\n",
    "print(f\"   Composite Score: {worst_q['composite_score']:.0f} (A:{worst_q['factual_accuracy_score']:.0f}, C:{worst_q['citation_compliance_score']:.0f}, R:{worst_q['retrieval_relevance_score']:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53838f0f",
   "metadata": {},
   "source": [
    "## üí° Actionable Improvement Recommendations\n",
    "\n",
    "Generate specific recommendations based on the top 3 agentic RAG metrics performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e89f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Agentic RAG Improvement Recommendations:\n",
      "============================================================\n",
      "1. üîç **Optimize Retrieval Relevance (Important - 30% weight)**: Improve LlamaIndex embeddings, document chunking strategy, or implement hybrid retrieval with keyword + semantic search.\n",
      "\n",
      "2. üéØ **Improve Retrieval Precision**: Too many irrelevant documents retrieved. Consider increasing similarity thresholds or improving document metadata and keywords.\n",
      "\n",
      "üîç Detailed Example Analysis:\n",
      "==============================\n",
      "Question: What is the deductible for collision coverage on auto insurance?\n",
      "Category: auto_insurance | Difficulty: easy\n",
      "\n",
      "Agent Answer: The deductible for collision coverage on auto insurance is $500 [Memory Context].\n",
      "Expected Answer: $500 deductible for collision coverage\n",
      "\n",
      "üìä Metric Breakdown:\n",
      "   Factual Accuracy: 100/100\n",
      "   Reasoning: The agent's answer accurately states the deductible for collision coverage on auto insurance as $500, which matches the correct answer provided.\n",
      "   \n",
      "Citation Compliance: 100/100\n",
      "   Status: Correct: Citations present when required\n",
      "   Citations Found: ['[Memory Context]']\n",
      "   \n",
      "Retrieval Relevance: 50/100\n",
      "   Precision: 0.333 | Recall: 1.000\n",
      "   Retrieved: ['AUTO_001', 'HOME_002', 'HEALTH_002']\n",
      "   Expected: ['AUTO_001']\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Composite Score: 85.0/100\n",
      "   Tokens Used: 420\n",
      "   Memory Sources: 2 | RAG Sources: 3\n"
     ]
    }
   ],
   "source": [
    "def generate_agentic_rag_recommendations(metrics: Dict[str, float], results_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Generate improvement recommendations based on agentic RAG evaluation results\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Factual Accuracy Recommendations (Metric 1 - 40% weight)\n",
    "    if metrics[\"avg_factual_accuracy\"] < 80:\n",
    "        recommendations.append(\n",
    "            \"üéØ **Improve Factual Accuracy (Critical - 40% weight)**: \"\n",
    "            \"Consider expanding the memory knowledge base, improving document quality, \"\n",
    "            \"or fine-tuning the LLM with insurance-specific training data.\"\n",
    "        )\n",
    "    \n",
    "    # Citation Compliance Recommendations (Metric 2 - 30% weight)\n",
    "    if metrics[\"avg_citation_compliance\"] < 80:\n",
    "        recommendations.append(\n",
    "            \"üìù **Enhance Citation Compliance (Important - 30% weight)**: \"\n",
    "            \"Modify prompts to consistently include source references. \"\n",
    "            \"Critical for regulatory compliance in insurance claims.\"\n",
    "        )\n",
    "    \n",
    "    # Retrieval Relevance Recommendations (Metric 3 - 30% weight)\n",
    "    if metrics[\"avg_retrieval_relevance\"] < 70:\n",
    "        recommendations.append(\n",
    "            \"üîç **Optimize Retrieval Relevance (Important - 30% weight)**: \"\n",
    "            \"Improve LlamaIndex embeddings, document chunking strategy, \"\n",
    "            \"or implement hybrid retrieval with keyword + semantic search.\"\n",
    "        )\n",
    "    \n",
    "    # Precision/Recall specific recommendations\n",
    "    if metrics[\"avg_retrieval_precision\"] < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üéØ **Improve Retrieval Precision**: \"\n",
    "            \"Too many irrelevant documents retrieved. Consider increasing similarity thresholds \"\n",
    "            \"or improving document metadata and keywords.\"\n",
    "        )\n",
    "    \n",
    "    if metrics[\"avg_retrieval_recall\"] < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìö **Improve Retrieval Recall**: \"\n",
    "            \"Missing relevant documents. Consider lowering similarity thresholds, \"\n",
    "            \"expanding document coverage, or using query expansion techniques.\"\n",
    "        )\n",
    "    \n",
    "    # Memory integration recommendations\n",
    "    if metrics[\"avg_memory_sources\"] < 1.0:\n",
    "        recommendations.append(\n",
    "            \"üß† **Enhance Memory Integration**: \"\n",
    "            \"Memory system underutilized. Improve memory retrieval algorithms \"\n",
    "            \"or expand the persistent knowledge base.\"\n",
    "        )\n",
    "    \n",
    "    # Token efficiency recommendations\n",
    "    if metrics[\"avg_tokens_per_question\"] > 1000:\n",
    "        recommendations.append(\n",
    "            \"üí∞ **Optimize Token Efficiency**: \"\n",
    "            \"High token usage detected. Consider shorter prompts, \"\n",
    "            \"better context filtering, or smaller model variants for cost optimization.\"\n",
    "        )\n",
    "    \n",
    "    # Category-specific recommendations\n",
    "    category_performance = results_df.groupby('category')['composite_score'].mean()\n",
    "    worst_categories = category_performance[category_performance < 70].index.tolist()\n",
    "    if worst_categories:\n",
    "        recommendations.append(\n",
    "            f\"üìä **Address Category Weaknesses**: \"\n",
    "            f\"Poor performance in {', '.join(worst_categories)}. \"\n",
    "            f\"Consider domain-specific training data or specialized retrieval strategies.\"\n",
    "        )\n",
    "    \n",
    "    # Success case recommendations\n",
    "    if metrics[\"avg_composite_score\"] >= 85:\n",
    "        recommendations.append(\n",
    "            \"üéâ **Strong Performance Detected**: \"\n",
    "            \"Agent performs well across all three metrics. Consider testing on \"\n",
    "            \"more complex scenarios, expanding to additional insurance domains, \"\n",
    "            \"or implementing A/B testing for production deployment.\"\n",
    "        )\n",
    "    \n",
    "    # Memory + RAG integration recommendations\n",
    "    if metrics[\"avg_rag_sources\"] > 2.5 and metrics[\"avg_memory_sources\"] < 1.0:\n",
    "        recommendations.append(\n",
    "            \"‚öñÔ∏è **Balance Memory and RAG Sources**: \"\n",
    "            \"Over-reliance on RAG retrieval vs. persistent memory. \"\n",
    "            \"Consider adjusting the memory retrieval scoring or expanding memory coverage.\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate comprehensive recommendations\n",
    "recommendations = generate_agentic_rag_recommendations(performance_metrics, results_df)\n",
    "\n",
    "print(\"üí° Agentic RAG Improvement Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\\n\")\n",
    "\n",
    "# Detailed example analysis\n",
    "print(\"üîç Detailed Example Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "sample_result = results_df.iloc[0]\n",
    "print(f\"Question: {sample_result['question']}\")\n",
    "print(f\"Category: {sample_result['category']} | Difficulty: {sample_result['difficulty']}\")\n",
    "print(f\"\\nAgent Answer: {sample_result['agent_answer']}\")\n",
    "print(f\"Expected Answer: {sample_result['correct_answer']}\")\n",
    "\n",
    "print(f\"\\nüìä Metric Breakdown:\")\n",
    "print(f\"   Factual Accuracy: {sample_result['factual_accuracy_score']:.0f}/100\")\n",
    "print(f\"   Reasoning: {sample_result['accuracy_reasoning']}\")\n",
    "print(f\"   \\nCitation Compliance: {sample_result['citation_compliance_score']:.0f}/100\")\n",
    "print(f\"   Status: {sample_result['compliance_status']}\")\n",
    "print(f\"   Citations Found: {sample_result['citations_found']}\")\n",
    "print(f\"   \\nRetrieval Relevance: {sample_result['retrieval_relevance_score']:.0f}/100\")\n",
    "print(f\"   Precision: {sample_result['retrieval_precision']:.3f} | Recall: {sample_result['retrieval_recall']:.3f}\")\n",
    "print(f\"   Retrieved: {sample_result['retrieved_doc_ids']}\")\n",
    "print(f\"   Expected: {sample_result['expected_doc_ids']}\")\n",
    "\n",
    "print(f\"\\nüéØ Overall Performance:\")\n",
    "print(f\"   Composite Score: {sample_result['composite_score']:.1f}/100\")\n",
    "print(f\"   Tokens Used: {sample_result['tokens_used']}\")\n",
    "print(f\"   Memory Sources: {sample_result['memory_sources_used']} | RAG Sources: {sample_result['rag_sources_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f51e36",
   "metadata": {},
   "source": [
    "## üéØ Summary & Key Learnings\n",
    "\n",
    "### ‚úÖ What We Demonstrated\n",
    "\n",
    "- ‚úÖ Integrated a memory-enabled insurance claims assistant with LlamaIndex RAG\n",
    "- ‚úÖ Implemented the top 3 agentic AI evaluation metrics used in production\n",
    "- ‚úÖ Evaluated insurance claims questions with comprehensive analytics\n",
    "- ‚úÖ Generated actionable improvement recommendations\n",
    "\n",
    "### üìä Top 3 Agentic RAG Metrics\n",
    "\n",
    "1. **Factual Accuracy** (40% weight) - LLM-based correctness scoring\n",
    "2. **Citation/Source Compliance** (30% weight) - Critical for insurance regulatory compliance\n",
    "3. **Retrieval Relevance** (30% weight) - LlamaIndex document retrieval quality with precision/recall\n",
    "\n",
    "### üîë Key Insights\n",
    "\n",
    "- **Memory + RAG Integration**: Persistent memory enhances RAG performance for insurance claims\n",
    "- **Compliance Requirements**: Citation compliance is critical for insurance applications\n",
    "- **Retrieval Quality**: LlamaIndex provides measurable precision/recall metrics\n",
    "- **Production Ready**: Framework supports A/B testing and continuous improvement\n",
    "\n",
    "### üöÄ Next Steps for Production\n",
    "\n",
    "1. **Expand Dataset**: Scale to 500+ questions across more insurance domains\n",
    "2. **Optimize Retrieval**: Implement hybrid keyword + semantic search\n",
    "3. **Memory Enhancement**: Expand persistent knowledge base with claims history\n",
    "4. **A/B Testing**: Compare different agent configurations\n",
    "5. **Real-time Monitoring**: Deploy evaluation metrics in production\n",
    "\n",
    "### üíº Production Considerations\n",
    "\n",
    "For a production insurance claims system, consider:\n",
    "- **Audit Trail**: Complete logging of all claim decisions with timestamps\n",
    "- **Compliance**: Ensure citation compliance meets regulatory requirements\n",
    "- **Performance**: Monitor token usage and optimize for cost efficiency\n",
    "- **Accuracy**: Maintain high factual accuracy to prevent claim errors\n",
    "- **Scalability**: Design for handling thousands of claims evaluations daily\n",
    "\n",
    "This evaluation framework demonstrates that agentic AI systems can be rigorously tested using industry-standard metrics, ensuring they meet production quality requirements for financial services! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
