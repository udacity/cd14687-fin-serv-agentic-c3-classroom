{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9783644c",
   "metadata": {},
   "source": [
    "# Concept 2: Evaluating AI Agents - Starter Notebook\n",
    "\n",
    "**Objective**: Learn to evaluate AI agents using industry-standard agentic RAG metrics.\n",
    "\n",
    "**Prerequisites**: Complete Concept 1 (Finance Memory Agent)\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "This notebook focuses on **evaluation methodology** rather than agent building. You'll implement:\n",
    "\n",
    "**Top 3 Agentic RAG Metrics:**\n",
    "- ðŸŽ¯ **Factual Accuracy** (40% weight): LLM-based correctness scoring\n",
    "- ðŸ“ **Citation/Source Compliance** (30% weight): Source attribution quality\n",
    "- ðŸ” **Retrieval Relevance** (30% weight): Document retrieval effectiveness with precision/recall\n",
    "\n",
    "**Evaluation Framework:**\n",
    "- Load and analyze the golden dataset (50 banking Q&A pairs)\n",
    "- Integrate with finance memory agent from Concept 1\n",
    "- Implement production-ready evaluation metrics\n",
    "- Generate performance insights and recommendations\n",
    "\n",
    "## ðŸ“Š About the Golden Dataset\n",
    "\n",
    "The dataset was generated using `data/generate_golden_dataset.py` and contains:\n",
    "\n",
    "**50 Banking Q&A Pairs** across categories:\n",
    "- Wire transfers, account benefits, deposits, fees, ATM services\n",
    "- Credit products, savings, lending, business services, security\n",
    "\n",
    "**Dataset Fields:**\n",
    "- `question_id`: Unique identifier (e.g., \"wire_001\")\n",
    "- `question`: Banking question for the agent\n",
    "- `correct_answer`: Ground truth response\n",
    "- `relevant_doc_ids`: Documents that should be retrieved\n",
    "- `category`: Banking domain for analysis\n",
    "- `difficulty`: easy/medium/hard for stratified evaluation\n",
    "- `should_have_citation`: Whether answer requires source attribution\n",
    "- `expected_retrieval_keywords`: Keywords for retrieval assessment\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4ac10",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup and Imports\n",
    "\n",
    "Import required libraries and configure the evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI for LLM\n",
    "from openai import OpenAI\n",
    "\n",
    "# LlamaIndex for RAG (you'll implement this)\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Configure LlamaIndex settings\n",
    "Settings.llm = LlamaOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://openai.vocareum.com/v1\"\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"ðŸ”§ Evaluation Environment Setup:\")\n",
    "print(f\"   âœ… OpenAI API Key: {'âœ“ Configured' if os.getenv('OPENAI_API_KEY') else 'âŒ Missing'}\")\n",
    "print(\"   ðŸ“Š Focus: Top 3 agentic RAG evaluation metrics\")\n",
    "print(\"   ðŸ¦ Domain: Banking policy Q&A evaluation\")\n",
    "print(\"   ðŸ“ˆ Dataset: 50 golden standard labeled examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25ebf9",
   "metadata": {},
   "source": [
    "## ðŸ“Š Load and Explore Golden Dataset\n",
    "\n",
    "Load the evaluation dataset generated by `data/generate_golden_dataset.py` and understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_golden_dataset() -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Load the golden Q&A dataset and policy documents\"\"\"\n",
    "    \n",
    "    # Load Q&A dataset\n",
    "    qa_data = []\n",
    "    with open('data/banking_qa_golden_dataset.csv', 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            qa_data.append({\n",
    "                'question_id': row['question_id'],\n",
    "                'question': row['question'],\n",
    "                'correct_answer': row['correct_answer'],\n",
    "                'relevant_doc_ids': row['relevant_doc_ids'].split('|') if row['relevant_doc_ids'] else [],\n",
    "                'category': row['category'],\n",
    "                'difficulty': row['difficulty'],\n",
    "                'should_have_citation': row['should_have_citation'].lower() == 'true',\n",
    "                'expected_retrieval_keywords': row['expected_retrieval_keywords'].split('|') if row['expected_retrieval_keywords'] else []\n",
    "            })\n",
    "    \n",
    "    # Load policy documents\n",
    "    with open('data/banking_policy_documents.json', 'r', encoding='utf-8') as f:\n",
    "        policy_docs = json.load(f)\n",
    "    \n",
    "    return qa_data, policy_docs\n",
    "\n",
    "# Load the datasets\n",
    "print(\"ðŸ“Š Loading Golden Evaluation Dataset...\")\n",
    "GOLDEN_QA_DATASET, POLICY_DOCUMENTS = load_golden_dataset()\n",
    "\n",
    "# Explore the dataset structure\n",
    "qa_df = pd.DataFrame(GOLDEN_QA_DATASET)\n",
    "\n",
    "print(\"\\nðŸ“‹ Dataset Overview:\")\n",
    "print(f\"   Total questions: {len(GOLDEN_QA_DATASET)}\")\n",
    "print(f\"   Policy documents: {len(POLICY_DOCUMENTS)}\")\n",
    "print(f\"   Categories: {qa_df['category'].nunique()}\")\n",
    "print(f\"   Difficulty levels: {sorted(qa_df['difficulty'].unique())}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Category Distribution:\")\n",
    "category_counts = qa_df['category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"   {category}: {count} questions\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Sample Question:\")\n",
    "sample_q = GOLDEN_QA_DATASET[0]\n",
    "print(f\"   ID: {sample_q['question_id']}\")\n",
    "print(f\"   Q: {sample_q['question']}\")\n",
    "print(f\"   A: {sample_q['correct_answer']}\")\n",
    "print(f\"   Category: {sample_q['category']} | Difficulty: {sample_q['difficulty']}\")\n",
    "print(f\"   Should cite: {sample_q['should_have_citation']}\")\n",
    "print(f\"   Relevant docs: {sample_q['relevant_doc_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b681712",
   "metadata": {},
   "source": [
    "## ðŸ§  Finance Memory Agent Integration\n",
    "\n",
    "**Provided**: Simplified version of the finance memory agent from Concept 1 for evaluation purposes.\n",
    "\n",
    "*Note: In a real implementation, you'd import the complete agent from Concept 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e20eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Finance Memory Agent for Evaluation\n",
    "# (In practice, this would be imported from Concept 1)\n",
    "\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "class MemoryEntry:\n",
    "    \"\"\"Memory entry for banking policy information\"\"\"\n",
    "    def __init__(self, topic: str, fact_text: str, source: str, weight: float = 1.0):\n",
    "        self.id = str(uuid.uuid4())\n",
    "        self.topic = topic\n",
    "        self.fact_text = fact_text\n",
    "        self.source = source\n",
    "        self.weight = weight\n",
    "        self.created_at = datetime.now()\n",
    "\n",
    "class FinanceMemoryManager:\n",
    "    \"\"\"Simplified memory manager from Concept 1\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memories = {}\n",
    "    \n",
    "    def add_banking_policies(self, policy_documents: List[Dict]):\n",
    "        \"\"\"Add banking policy documents to memory\"\"\"\n",
    "        for doc in policy_documents:\n",
    "            memory = MemoryEntry(\n",
    "                topic=doc['category'],\n",
    "                fact_text=f\"{doc['title']}: {doc['content']}\",\n",
    "                source=doc['doc_id'],\n",
    "                weight=2.0\n",
    "            )\n",
    "            self.memories[memory.id] = memory\n",
    "    \n",
    "    def retrieve_memories(self, query: str, top_k: int = 2) -> List[MemoryEntry]:\n",
    "        \"\"\"Simple keyword-based memory retrieval\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        scored_memories = []\n",
    "        \n",
    "        for memory in self.memories.values():\n",
    "            score = 0\n",
    "            fact_lower = memory.fact_text.lower()\n",
    "            for word in query_lower.split():\n",
    "                if len(word) > 3 and word in fact_lower:\n",
    "                    score += memory.weight\n",
    "            if score > 0:\n",
    "                scored_memories.append((memory, score))\n",
    "        \n",
    "        scored_memories.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [mem for mem, score in scored_memories[:top_k]]\n",
    "\n",
    "class EvaluationAgent:\n",
    "    \"\"\"Finance assistant with memory + RAG for evaluation\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_manager: FinanceMemoryManager, rag_retriever):\n",
    "        self.memory = memory_manager\n",
    "        self.rag_retriever = rag_retriever\n",
    "        self.client = client\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Answer using both memory and RAG retrieval\"\"\"\n",
    "        \n",
    "        # Get memory context\n",
    "        memory_results = self.memory.retrieve_memories(question, top_k=2)\n",
    "        memory_context = \"\\n\".join([f\"Memory: {mem.fact_text}\" for mem in memory_results])\n",
    "        \n",
    "        # Get RAG context\n",
    "        rag_results = self.rag_retriever.retrieve(question)\n",
    "        rag_context = \"\\n\".join([f\"Document: {node.text}\" for node in rag_results])\n",
    "        \n",
    "        # Combine contexts\n",
    "        combined_context = f\"Memory Context:\\n{memory_context}\\n\\nDocument Context:\\n{rag_context}\"\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"\n",
    "You are a banking policy assistant with access to memory and documents.\n",
    "\n",
    "Context:\\n{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide accurate answers based on the context\n",
    "- Include source references in [brackets] for factual claims\n",
    "- If information is missing, say \"I don't have that information\"\n",
    "- Keep responses professional and concise\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"memory_sources\": [mem.source for mem in memory_results],\n",
    "                \"rag_sources\": [node.metadata.get('source', 'unknown') for node in rag_results],\n",
    "                \"memory_count\": len(memory_results),\n",
    "                \"rag_count\": len(rag_results),\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"retrieved_nodes\": rag_results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"memory_sources\": [], \"rag_sources\": [],\n",
    "                \"memory_count\": 0, \"rag_count\": 0,\n",
    "                \"tokens_used\": 0, \"retrieved_nodes\": []\n",
    "            }\n",
    "\n",
    "print(\"ðŸ§  Finance Memory Agent Components Ready:\")\n",
    "print(\"   âœ… MemoryEntry and FinanceMemoryManager\")\n",
    "print(\"   âœ… EvaluationAgent with memory + RAG integration\")\n",
    "print(\"   ðŸ”— Ready for evaluation framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d98b09",
   "metadata": {},
   "source": [
    "## ðŸ”  LlamaIndex RAG Setup\n",
    "\n",
    "**Provided**: LlamaIndex RAG system initialization with banking policy documents.\n",
    "\n",
    "**Focus**: This setup is provided so you can focus on learning evaluation methodology rather than RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llamaindex_rag(policy_documents: List[Dict]) -> VectorIndexRetriever:\n",
    "    \"\"\"Set up LlamaIndex RAG system with banking policy documents\"\"\"\n",
    "    \n",
    "    # Convert policy documents to LlamaIndex Document objects\n",
    "    documents = []\n",
    "    for doc in policy_documents:\n",
    "        # Create document with metadata\n",
    "        document = Document(\n",
    "            text=f\"{doc['title']}\\n\\n{doc['content']}\",\n",
    "            metadata={\n",
    "                'doc_id': doc['doc_id'],\n",
    "                'title': doc['title'],\n",
    "                'category': doc['category'],\n",
    "                'source': doc['doc_id'],\n",
    "                'keywords': ','.join(doc['relevance_keywords'])\n",
    "            }\n",
    "        )\n",
    "        documents.append(document)\n",
    "    \n",
    "    # Create vector index\n",
    "    print(\"   ðŸ” Building LlamaIndex vector store...\")\n",
    "    vector_index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=vector_index,\n",
    "        similarity_top_k=3  # Retrieve top 3 most relevant documents\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… LlamaIndex RAG initialized with {len(documents)} documents\")\n",
    "    return retriever\n",
    "\n",
    "# Set up the RAG system\n",
    "print(\"ðŸ” Setting up LlamaIndex RAG...\")\n",
    "llamaindex_retriever = setup_llamaindex_rag(POLICY_DOCUMENTS)\n",
    "\n",
    "# Initialize memory manager and agent\n",
    "print(\"ðŸ§  Initializing Finance Memory Manager...\")\n",
    "memory_manager = FinanceMemoryManager()\n",
    "memory_manager.add_banking_policies(POLICY_DOCUMENTS)\n",
    "evaluation_agent = EvaluationAgent(memory_manager, llamaindex_retriever)\n",
    "\n",
    "print(\"\\nâœ… RAG System Ready!\")\n",
    "print(f\"   ðŸ“„ Indexed documents: {len(POLICY_DOCUMENTS)}\")\n",
    "print(f\"   ðŸ§  Memory entries: {len(memory_manager.memories)}\")\n",
    "print(f\"   ðŸ¤– Evaluation agent: Ready for testing\")\n",
    "\n",
    "# Test the integrated system\n",
    "print(f\"\\nðŸ§ª Testing Integrated System:\")\n",
    "test_question = \"What's the cut-off time for same-day domestic wire transfers?\"\n",
    "test_response = evaluation_agent.answer_question(test_question)\n",
    "\n",
    "print(f\"   Question: {test_response['question']}\")\n",
    "print(f\"   Answer: {test_response['answer']}\")\n",
    "print(f\"   Memory sources: {test_response['memory_count']} entries\")\n",
    "print(f\"   RAG sources: {test_response['rag_count']} documents\")\n",
    "print(f\"   Tokens used: {test_response['tokens_used']}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready to implement evaluation metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b3bb",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ TODO Section 1: Implement Factual Accuracy Metric\n",
    "\n",
    "**YOUR TASK**: Implement the first metric - Factual Accuracy (40% weight).\n",
    "\n",
    "**Learning Focus**: LLM-based evaluation for correctness assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a450b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_factual_accuracy(agent_answer: str, correct_answer: str, question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Metric 1: Factual Accuracy (40% weight) - LLM-based correctness scoring\"\"\"\n",
    "    \n",
    "    # TODO: Create an evaluation prompt for LLM-based scoring\n",
    "    # Scoring scale: 0-100 with specific ranges:\n",
    "    # - 90-100: All key facts correct, comprehensive\n",
    "    # - 70-89: Most facts correct, minor missing details  \n",
    "    # - 50-69: Some correct facts, significant gaps\n",
    "    # - 30-49: Few correct facts, mostly incorrect\n",
    "    # - 0-29: Incorrect or completely missing information\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    # YOUR CODE HERE: Create evaluation prompt\n",
    "    # Include: question, correct_answer, agent_answer\n",
    "    # Request: JSON response with accuracy_score, reasoning, key_facts_missing\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Create factual accuracy evaluation prompt\")\n",
    "    \n",
    "    try:\n",
    "        # TODO: Call OpenAI API for evaluation\n",
    "        # YOUR CODE HERE: Use client.chat.completions.create()\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Implement OpenAI API call for evaluation\")\n",
    "        \n",
    "        # TODO: Parse JSON response\n",
    "        # YOUR CODE HERE: Extract accuracy_score, reasoning, key_facts_missing\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Parse evaluation response\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"accuracy_score\": 0,\n",
    "            \"accuracy_reasoning\": f\"Evaluation error: {str(e)}\",\n",
    "            \"key_facts_missing\": []\n",
    "        }\n",
    "\n",
    "# Test the function when implemented\n",
    "print(\"ðŸŽ¯ Factual Accuracy Metric:\")\n",
    "print(\"   âš ï¸ Complete the TODO sections above to test this metric!\")\n",
    "\n",
    "# Example test (uncomment after implementation):\n",
    "# test_result = evaluate_factual_accuracy(\n",
    "#     \"The cutoff is 2 PM EST\", \n",
    "#     \"2:00 PM EST\", \n",
    "#     \"What's the wire transfer cutoff?\"\n",
    "# )\n",
    "# print(f\"   Test Score: {test_result['accuracy_score']}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93710521",
   "metadata": {},
   "source": [
    "## ðŸ“ TODO Section 2: Implement Citation Compliance Metric\n",
    "\n",
    "**YOUR TASK**: Implement the second metric - Citation/Source Compliance (30% weight).\n",
    "\n",
    "**Learning Focus**: Pattern matching and compliance checking for financial regulatory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_citation_compliance(answer: str, should_have_citation: bool, sources_used: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Metric 2: Citation/Source Compliance (30% weight) - Source attribution quality\"\"\"\n",
    "    \n",
    "    # TODO: Define citation patterns to look for\n",
    "    # Examples: [Document: title], [source], \"according to\", \"policy states\"\n",
    "    citation_patterns = [\n",
    "        # YOUR CODE HERE: Add regex patterns for citation detection\n",
    "        # r'\\[.*?\\]',  # [Document: title] or [source]\n",
    "        # Add more patterns...\n",
    "    ]\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Define citation patterns\")\n",
    "    \n",
    "    # TODO: Search for citations in the answer\n",
    "    citations_found = []\n",
    "    # YOUR CODE HERE: Use re.findall() to find citation patterns\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement citation detection\")\n",
    "    \n",
    "    has_citations = len(citations_found) > 0\n",
    "    \n",
    "    # TODO: Calculate compliance score based on requirements\n",
    "    # Rules:\n",
    "    # - Required + Present = 100 points\n",
    "    # - Not Required + Not Present = 100 points  \n",
    "    # - Required + Missing = 0 points\n",
    "    # - Not Required + Present = 80 points (acceptable)\n",
    "    \n",
    "    # YOUR CODE HERE: Implement compliance scoring logic\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement compliance scoring\")\n",
    "    \n",
    "    # TODO: Add bonus points for citing correct sources\n",
    "    # Check if source IDs appear in the answer text\n",
    "    source_accuracy_bonus = 0\n",
    "    # YOUR CODE HERE: Calculate source accuracy bonus\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement source accuracy bonus\")\n",
    "    \n",
    "    final_score = min(100, compliance_score + source_accuracy_bonus)\n",
    "    \n",
    "    return {\n",
    "        \"citation_compliance_score\": final_score,\n",
    "        \"citations_found\": citations_found,\n",
    "        \"citation_expected\": should_have_citation,\n",
    "        \"citation_present\": has_citations,\n",
    "        \"compliance_status\": compliance_status,\n",
    "        \"source_accuracy_bonus\": source_accuracy_bonus\n",
    "    }\n",
    "\n",
    "print(\"ðŸ“ Citation Compliance Metric:\")\n",
    "print(\"   âš ï¸ Complete the TODO sections above to test this metric!\")\n",
    "\n",
    "# Example test (uncomment after implementation):\n",
    "# test_result = evaluate_citation_compliance(\n",
    "#     \"The cutoff is 2 PM EST [wire_transfer_policy]\", \n",
    "#     True, \n",
    "#     [\"wire_transfer_policy\"]\n",
    "# )\n",
    "# print(f\"   Test Score: {test_result['citation_compliance_score']}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dfac1",
   "metadata": {},
   "source": [
    "## ðŸ” TODO Section 3: Implement Retrieval Relevance Metric\n",
    "\n",
    "**YOUR TASK**: Implement the third metric - Retrieval Relevance (30% weight).\n",
    "\n",
    "**Learning Focus**: Precision/recall analysis for RAG retrieval quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_relevance(question: str, retrieved_nodes: List, expected_doc_ids: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Metric 3: Retrieval Relevance (30% weight) - Quality of document retrieval\"\"\"\n",
    "    \n",
    "    if not retrieved_nodes:\n",
    "        return {\n",
    "            \"retrieval_relevance_score\": 0,\n",
    "            \"retrieved_doc_ids\": [],\n",
    "            \"expected_doc_ids\": expected_doc_ids,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"relevance_reasoning\": \"No documents retrieved\"\n",
    "        }\n",
    "    \n",
    "    # TODO: Extract document IDs from retrieved nodes\n",
    "    retrieved_doc_ids = []\n",
    "    # YOUR CODE HERE: Get doc_id from node.metadata for each node\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Extract retrieved document IDs\")\n",
    "    \n",
    "    # TODO: Calculate precision and recall\n",
    "    if expected_doc_ids:\n",
    "        expected_set = set(expected_doc_ids)\n",
    "        retrieved_set = set(retrieved_doc_ids)\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate precision and recall\n",
    "        # Precision = relevant_retrieved / total_retrieved\n",
    "        # Recall = relevant_retrieved / total_relevant\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Calculate precision and recall\")\n",
    "        \n",
    "        # TODO: Calculate F1-score as overall relevance measure\n",
    "        # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate F1-score\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Calculate F1-score\")\n",
    "        \n",
    "        # Convert to 0-100 scale\n",
    "        relevance_score = f1_score * 100\n",
    "        reasoning = f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1_score:.2f}\"\n",
    "        \n",
    "    else:\n",
    "        # Handle questions with no expected documents\n",
    "        precision = 0.0\n",
    "        recall = 1.0 if not retrieved_doc_ids else 0.0\n",
    "        relevance_score = 100 if not retrieved_doc_ids else 50\n",
    "        reasoning = \"No expected documents for this question\"\n",
    "    \n",
    "    return {\n",
    "        \"retrieval_relevance_score\": relevance_score,\n",
    "        \"retrieved_doc_ids\": retrieved_doc_ids,\n",
    "        \"expected_doc_ids\": expected_doc_ids,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"relevance_reasoning\": reasoning\n",
    "    }\n",
    "\n",
    "print(\"ðŸ” Retrieval Relevance Metric:\")\n",
    "print(\"   âš ï¸ Complete the TODO sections above to test this metric!\")\n",
    "\n",
    "# Example test (uncomment after implementation):\n",
    "# # Mock retrieved nodes for testing\n",
    "# class MockNode:\n",
    "#     def __init__(self, doc_id):\n",
    "#         self.metadata = {'doc_id': doc_id}\n",
    "# \n",
    "# test_nodes = [MockNode('wire_transfer_policy'), MockNode('atm_network_fees')]\n",
    "# test_result = evaluate_retrieval_relevance(\n",
    "#     \"What's the wire transfer cutoff?\",\n",
    "#     test_nodes,\n",
    "#     ['wire_transfer_policy']\n",
    "# )\n",
    "# print(f\"   Test Score: {test_result['retrieval_relevance_score']:.0f}/100\")\n",
    "# print(f\"   Precision: {test_result['precision']:.2f} | Recall: {test_result['recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84420b0",
   "metadata": {},
   "source": [
    "## ðŸ“Š TODO Section 4: Complete Evaluation Framework\n",
    "\n",
    "**YOUR TASK**: Combine all three metrics into a complete evaluation framework.\n",
    "\n",
    "**Learning Focus**: Weighted scoring and comprehensive agent assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df38726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGEvaluationFramework:\n",
    "    \"\"\"Complete evaluation framework for agentic RAG systems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = client\n",
    "    \n",
    "    def evaluate_complete_response(self, agent_response: Dict[str, Any], gold_item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate agent response using all three metrics\"\"\"\n",
    "        \n",
    "        # TODO: Use the three evaluation functions you implemented above\n",
    "        \n",
    "        # Metric 1: Factual Accuracy (40% weight)\n",
    "        # YOUR CODE HERE: Call evaluate_factual_accuracy()\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Implement factual accuracy evaluation\")\n",
    "        \n",
    "        # Metric 2: Citation Compliance (30% weight)  \n",
    "        # YOUR CODE HERE: Call evaluate_citation_compliance()\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Implement citation compliance evaluation\")\n",
    "        \n",
    "        # Metric 3: Retrieval Relevance (30% weight)\n",
    "        # YOUR CODE HERE: Call evaluate_retrieval_relevance()\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Implement retrieval relevance evaluation\")\n",
    "        \n",
    "        # TODO: Calculate weighted composite score\n",
    "        # Weights: Accuracy 40%, Citation 30%, Retrieval 30%\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate composite score\n",
    "        \n",
    "        raise NotImplementedError(\"TODO: Calculate weighted composite score\")\n",
    "        \n",
    "        return {\n",
    "            # Question info\n",
    "            \"question_id\": gold_item[\"question_id\"],\n",
    "            \"question\": gold_item[\"question\"],\n",
    "            \"category\": gold_item[\"category\"],\n",
    "            \"difficulty\": gold_item[\"difficulty\"],\n",
    "            \n",
    "            # Answers\n",
    "            \"agent_answer\": agent_response[\"answer\"],\n",
    "            \"correct_answer\": gold_item[\"correct_answer\"],\n",
    "            \n",
    "            # Metric scores\n",
    "            \"factual_accuracy_score\": accuracy_eval[\"accuracy_score\"],\n",
    "            \"citation_compliance_score\": citation_eval[\"citation_compliance_score\"],\n",
    "            \"retrieval_relevance_score\": retrieval_eval[\"retrieval_relevance_score\"],\n",
    "            \n",
    "            # Overall performance\n",
    "            \"composite_score\": composite_score,\n",
    "            \"tokens_used\": agent_response[\"tokens_used\"]\n",
    "        }\n",
    "\n",
    "print(\"ðŸ“Š Complete Evaluation Framework:\")\n",
    "print(\"   âš ï¸ Complete all TODO sections above to use this framework!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118f383",
   "metadata": {},
   "source": [
    "## ðŸ§ª TODO Section 5: Run Evaluation Suite\n",
    "\n",
    "**YOUR TASK**: Run the complete evaluation on a subset of questions.\n",
    "\n",
    "**Learning Focus**: End-to-end evaluation process and results analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the evaluation framework and run evaluation\n",
    "\n",
    "# evaluator = AgenticRAGEvaluationFramework()\n",
    "# evaluation_results = []\n",
    "\n",
    "# TODO: Evaluate first 5 questions for learning purposes\n",
    "# evaluation_subset = GOLDEN_QA_DATASET[:5]\n",
    "\n",
    "# print(\"ðŸ§ª Running Evaluation Suite...\")\n",
    "# print(\"=\" * 40)\n",
    "\n",
    "# for i, gold_item in enumerate(evaluation_subset, 1):\n",
    "#     print(f\"[{i}/{len(evaluation_subset)}] Evaluating: {gold_item['question_id']}\")\n",
    "#     \n",
    "#     # TODO: Get agent response\n",
    "#     # YOUR CODE HERE: Use evaluation_agent.answer_question()\n",
    "#     \n",
    "#     # TODO: Evaluate the response\n",
    "#     # YOUR CODE HERE: Use evaluator.evaluate_complete_response()\n",
    "#     \n",
    "#     # TODO: Store results and show progress\n",
    "#     # YOUR CODE HERE: Append to evaluation_results and print scores\n",
    "\n",
    "print(\"âš ï¸ Complete all previous TODO sections to run the evaluation!\")\n",
    "print(\"\\nðŸ“‹ When completed, you'll see:\")\n",
    "print(\"   - Individual question scores across all 3 metrics\")\n",
    "print(\"   - Composite scores with weighted averages\")\n",
    "print(\"   - Performance breakdown by category and difficulty\")\n",
    "print(\"   - Token usage and efficiency metrics\")\n",
    "\n",
    "# After completing this section, you can analyze results with:\n",
    "print(\"\\nðŸ’¡ Optional: Analyze Results\")\n",
    "print(\"   results_df = pd.DataFrame(evaluation_results)\")\n",
    "print(\"   print(results_df[['question_id', 'factual_accuracy_score', 'citation_compliance_score', 'retrieval_relevance_score', 'composite_score']])\")\n",
    "print(\"   print(f'Average Composite Score: {results_df[\\\"composite_score\\\"].mean():.1f}/100')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d465ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_results(results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze evaluation results for insights and recommendations\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸ No evaluation results available. Complete previous sections first!\")\n",
    "        return {}\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # TODO: Calculate performance metrics\n",
    "    performance_metrics = {\n",
    "        # YOUR CODE HERE: Calculate averages for each metric\n",
    "        # \"avg_factual_accuracy\": results_df['factual_accuracy_score'].mean(),\n",
    "        # \"avg_citation_compliance\": ...\n",
    "        # \"avg_retrieval_relevance\": ...\n",
    "        # \"avg_composite_score\": ...\n",
    "    }\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Calculate performance metrics\")\n",
    "    \n",
    "    # TODO: Analyze performance by category\n",
    "    # YOUR CODE HERE: Group by category and calculate mean scores\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement category analysis\")\n",
    "    \n",
    "    # TODO: Analyze performance by difficulty\n",
    "    # YOUR CODE HERE: Group by difficulty and calculate mean scores\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement difficulty analysis\")\n",
    "    \n",
    "    # TODO: Identify problem areas\n",
    "    # YOUR CODE HERE: Find questions with low scores in each metric\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement problem area identification\")\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "def generate_improvement_recommendations(metrics: Dict) -> List[str]:\n",
    "    \"\"\"Generate actionable recommendations based on evaluation results\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # TODO: Generate recommendations based on metric performance\n",
    "    # Check each metric and suggest improvements:\n",
    "    \n",
    "    # if metrics.get(\"avg_factual_accuracy\", 0) < 80:\n",
    "    #     recommendations.append(\"ðŸŽ¯ Improve Factual Accuracy: ...\")\n",
    "    \n",
    "    # YOUR CODE HERE: Add more recommendation logic\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: Implement recommendation generation\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# TODO: Run analysis when evaluation results are available\n",
    "print(\"ðŸ“ˆ Performance Analysis Framework Ready:\")\n",
    "print(\"   âš ï¸ Complete evaluation first to generate insights!\")\n",
    "\n",
    "# Uncomment when evaluation is complete:\n",
    "# metrics = analyze_evaluation_results(evaluation_results)\n",
    "# recommendations = generate_improvement_recommendations(metrics)\n",
    "# \n",
    "# print(\"\\nðŸ’¡ Improvement Recommendations:\")\n",
    "# for i, rec in enumerate(recommendations, 1):\n",
    "#     print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06033574",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary & Next Steps\n",
    "\n",
    "**Congratulations!** You've learned to implement the top 3 agentic RAG evaluation metrics used in production.\n",
    "\n",
    "### ðŸ† What You've Accomplished\n",
    "\n",
    "By completing the TODO sections, you've built:\n",
    "\n",
    "1. **ðŸŽ¯ Factual Accuracy Evaluator** (40% weight)\n",
    "   - LLM-based correctness scoring against golden standards\n",
    "   - Critical for reliable financial advice systems\n",
    "\n",
    "2. **ðŸ“ Citation Compliance Checker** (30% weight)\n",
    "   - Source attribution quality assessment\n",
    "   - Essential for regulatory compliance in financial services\n",
    "\n",
    "3. **ðŸ” Retrieval Relevance Analyzer** (30% weight)\n",
    "   - Precision/recall metrics for RAG document retrieval\n",
    "   - Validates that the right information is found for questions\n",
    "\n",
    "4. **ðŸ“Š Complete Evaluation Framework**\n",
    "   - Weighted composite scoring with industry-standard weights\n",
    "   - Performance analysis by category and difficulty\n",
    "   - Actionable improvement recommendations\n",
    "\n",
    "### ðŸ’¡ Key Learning Insights\n",
    "\n",
    "- **Memory + RAG Integration**: How persistent memory enhances retrieval performance\n",
    "- **Production Metrics**: Industry-standard evaluation approaches for agentic AI\n",
    "- **Financial Compliance**: Citation requirements for banking applications\n",
    "- **Evaluation Best Practices**: Structured assessment for continuous improvement\n",
    "\n",
    "### ðŸš€ Production Deployment Considerations\n",
    "\n",
    "Your evaluation framework is ready for:\n",
    "\n",
    "1. **Continuous Monitoring**: Deploy metrics in production for real-time assessment\n",
    "2. **A/B Testing**: Compare different agent configurations objectively  \n",
    "3. **Regulatory Compliance**: Meet financial services requirements for audit trails\n",
    "4. **Cost Optimization**: Monitor token usage and efficiency for budget control\n",
    "5. **Quality Assurance**: Ensure consistent performance across banking domains\n",
    "\n",
    "### ðŸ“ˆ Next Steps for Advanced Implementation\n",
    "\n",
    "1. **Scale Dataset**: Expand to 500+ questions across more banking domains\n",
    "2. **Hybrid Retrieval**: Implement keyword + semantic search combinations\n",
    "3. **Advanced Memory**: Enhance persistent knowledge base from Concept 1\n",
    "4. **Real-time Evaluation**: Deploy metrics for production monitoring\n",
    "5. **Multi-agent Systems**: Evaluate collaborative agent architectures\n",
    "\n",
    "### ðŸ”— Integration with Concept 1\n",
    "\n",
    "This evaluation framework validates that your finance memory agent from Concept 1 performs well when enhanced with RAG capabilities, demonstrating the value of persistent memory in agentic AI systems!\n",
    "\n",
    "**Ready for Production**: Your evaluation system uses the same metrics employed by leading financial institutions for AI system assessment. ðŸ¦âœ¨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
